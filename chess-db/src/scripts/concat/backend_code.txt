# File Concatenation Report
# Generated: Sat Dec 14 06:22:09 PM MST 2024
# Directory: /home/ameer/Sync/github/ECE501C/chess-db/src

# Patterns:
#   Include: py
#   Exclude: 
#   Ignore: 

### File: ./backend/database.py
### Size: 1097 bytes
### Content:
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, declarative_base
from sqlalchemy import text
import os
from dotenv import load_dotenv

load_dotenv()
engine = create_async_engine(os.getenv("DATABASE_URL"), echo=True)
SessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)
Base = declarative_base()
async def init_db():
    """Initialize database tables if they don't exist"""
    async with engine.begin() as conn:
        # Get list of existing tables using SQLAlchemy's text construct
        result = await conn.execute(
            text("""
                SELECT tablename 
                FROM pg_tables 
                WHERE schemaname = 'public'
            """)
        )
        existing_tables = {row[0] for row in result}
        
        # Only create tables that don't exist
        if not existing_tables:
            await conn.run_sync(Base.metadata.create_all)
            print("Created database tables.")
        else:
            print("Database tables already exist, skipping creation.")
### End of ./backend/database.py

### File: ./backend/models.py
### Size: 10360 bytes
### Content:
from pydantic import BaseModel, ConfigDict
from datetime import datetime
from typing import Optional
from sqlalchemy import Column, Integer, String, Text, DateTime
from sqlalchemy import Column, Integer, String, Text, DateTime, Date, ForeignKey
from sqlalchemy.orm import relationship
from database import Base

# SQLAlchemy Model
class ItemDB(Base):
    __tablename__ = "items"

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String(100), nullable=False)
    description = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)

# Pydantic Models for API
class ItemBase(BaseModel):
    name: str
    description: Optional[str] = None

class ItemCreate(ItemBase):
    pass

class ItemUpdate(ItemBase):
    name: Optional[str] = None

class ItemResponse(ItemBase):
    id: int
    created_at: datetime
    model_config = ConfigDict(from_attributes=True)

class PlayerResponse(BaseModel):
    id: int
    name: str
    model_config = ConfigDict(from_attributes=True)

class PlayerDB(Base):
    __tablename__ = "players"

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, nullable=False, unique=True)

class GameDB(Base):
    __tablename__ = "games"

    id = Column(Integer, primary_key=True, index=True)
    white_player_id = Column(Integer, ForeignKey("players.id"))
    black_player_id = Column(Integer, ForeignKey("players.id"))
    date = Column(Date)
    result = Column(String(10))
    eco = Column(String(10))
    moves = Column(Text)
    
    white_player = relationship("PlayerDB", foreign_keys=[white_player_id])
    black_player = relationship("PlayerDB", foreign_keys=[black_player_id])

# Pydantic Models for API
class GameResponse(BaseModel):
    id: int
    white_player_id: int
    black_player_id: int
    white_player: Optional[PlayerResponse] = None
    black_player: Optional[PlayerResponse] = None
    date: Optional[datetime] = None
    result: str
    eco: Optional[str] = None
    moves: str
    model_config = ConfigDict(from_attributes=True)



from pydantic import BaseModel, Field, ConfigDict
from typing import Optional, List
from datetime import datetime

class MoveCountAnalysis(BaseModel):
    """Statistical analysis of move counts across chess games"""
    actual_full_moves: int = Field(
        ...,
        description="Number of full moves in game",
        ge=0,
        le=500
    )
    number_of_games: int = Field(
        ...,
        description="Count of games with this move count",
        ge=0
    )
    avg_bytes: float = Field(
        ...,
        description="Average size of encoded game data in bytes",
        ge=0
    )
    results: str = Field(
        ...,
        description="Aggregated game results for this move count"
    )
    min_stored_count: Optional[int] = Field(
        None,
        description="Minimum stored move count",
        ge=0
    )
    max_stored_count: Optional[int] = Field(
        None,
        description="Maximum stored move count",
        ge=0
    )
    avg_stored_count: float = Field(
        ...,
        description="Average stored move count",
        ge=0
    )

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "actual_full_moves": 40,
                "number_of_games": 1000,
                "avg_bytes": 156.5,
                "results": "1-0, 1/2-1/2, 0-1",
                "min_stored_count": 35,
                "max_stored_count": 45,
                "avg_stored_count": 40.5
            }
        }
    )

class PlayerPerformanceResponse(BaseModel):
    """Player performance statistics over a time period"""
    time_period: str = Field(
        ...,
        description="Time period (month/year)",
        examples=["2024-01", "2024"]
    )
    games_played: int = Field(
        ...,
        description="Total games played in period",
        ge=0
    )
    wins: int = Field(
        ...,
        description="Number of wins",
        ge=0
    )
    losses: int = Field(
        ...,
        description="Number of losses",
        ge=0
    )
    draws: int = Field(
        ...,
        description="Number of draws",
        ge=0
    )
    win_rate: float = Field(
        ...,
        description="Win percentage",
        ge=0,
        le=100
    )
    avg_moves: float = Field(
        ...,
        description="Average moves per game",
        ge=0
    )
    white_games: int = Field(
        ...,
        description="Games played as white",
        ge=0
    )
    black_games: int = Field(
        ...,
        description="Games played as black",
        ge=0
    )
    elo_rating: Optional[int] = Field(
        None,
        description="ELO rating if available",
        ge=0,
        le=3000
    )

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "time_period": "2024-01",
                "games_played": 50,
                "wins": 25,
                "losses": 15,
                "draws": 10,
                "win_rate": 62.5,
                "avg_moves": 45.3,
                "white_games": 26,
                "black_games": 24,
                "elo_rating": 2100
            }
        }
    )

    def __init__(self, **data):
        super().__init__(**data)
        # Validate that wins + losses + draws equals games_played
        if self.wins + self.losses + self.draws != self.games_played:
            raise ValueError("Sum of wins, losses, and draws must equal games_played")
        # Validate that white_games + black_games equals games_played
        if self.white_games + self.black_games != self.games_played:
            raise ValueError("Sum of white_games and black_games must equal games_played")

class OpeningStatsResponse(BaseModel):
    """Statistical analysis of player's performance with specific openings"""
    eco_code: str = Field(
        ...,
        description="ECO code of the opening",
        min_length=3,
        max_length=3,
        pattern="^[A-E][0-9]{2}$"  # Updated from regex to pattern
    )
    opening_name: str = Field(
        ...,
        description="Name of the opening",
        min_length=1
    )
    games_played: int = Field(
        ...,
        description="Number of games with this opening",
        ge=0
    )
    win_rate: float = Field(
        ...,
        description="Win rate with this opening",
        ge=0,
        le=100
    )
    avg_moves: float = Field(
        ...,
        description="Average game length with this opening",
        ge=0
    )

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "eco_code": "E04",
                "opening_name": "Catalan Opening",
                "games_played": 150,
                "win_rate": 58.5,
                "avg_moves": 42.7
            }
        }
    )




from pydantic import BaseModel, Field, ConfigDict
from typing import Optional, List
from datetime import datetime

class OpeningAnalysis(BaseModel):
    """Analysis of a player's performance with a specific opening"""
    eco_code: str = Field(..., description="ECO code of the opening")
    games_played: int = Field(..., description="Number of games with this opening")
    wins: int = Field(..., description="Number of wins with this opening")
    draws: int = Field(..., description="Number of draws with this opening")
    win_rate: float = Field(..., description="Win percentage with this opening")
    performance_score: float = Field(..., description="Overall performance score")
    avg_moves: float = Field(..., description="Average game length with this opening")
    played_white: bool = Field(..., description="Has played this opening as white")
    played_black: bool = Field(..., description="Has played this opening as black")

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "eco_code": "B07",
                "games_played": 42,
                "wins": 25,
                "draws": 10,
                "win_rate": 59.52,
                "performance_score": 71.43,
                "avg_moves": 35.8,
                "played_white": True,
                "played_black": True
            }
        }
    )

class DetailedPerformanceResponse(BaseModel):
    """Enhanced player performance statistics over a time period"""
    time_period: str = Field(..., description="Time period of analysis")
    games_played: int = Field(..., description="Total games played")
    wins: int = Field(..., description="Number of wins")
    losses: int = Field(..., description="Number of losses")
    draws: int = Field(..., description="Number of draws")
    win_rate: float = Field(..., description="Win percentage")
    avg_moves: float = Field(..., description="Average moves per game")
    white_games: int = Field(..., description="Games played as white")
    black_games: int = Field(..., description="Games played as black")
    avg_elo: Optional[int] = Field(None, description="Average ELO rating")
    elo_change: Optional[int] = Field(None, description="ELO rating change")
    opening_diversity: float = Field(..., description="Opening diversity score")
    avg_game_length: float = Field(..., description="Average game length")

    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "time_period": "2024-01",
                "games_played": 50,
                "wins": 25,
                "losses": 15,
                "draws": 10,
                "win_rate": 62.5,
                "avg_moves": 45.3,
                "white_games": 26,
                "black_games": 24,
                "avg_elo": 2100,
                "elo_change": 15,
                "opening_diversity": 0.85,
                "avg_game_length": 42.7
            }
        }
    )

class AnalyticsParameters(BaseModel):
    """Parameters for analytics queries"""
    time_grouping: str = Field(
        "month",
        description="Time period grouping (day, week, month, year)"
    )
    start_date: Optional[str] = Field(None, description="Start date for analysis")
    end_date: Optional[str] = Field(None, description="End date for analysis")
    min_games: Optional[int] = Field(5, description="Minimum games threshold")
    time_period: Optional[str] = Field(
        None,
        description="Time period filter (1y, 6m, 3m, 1m)"
    )
### End of ./backend/models.py

### File: ./backend/main.py
### Size: 8754 bytes
### Content:
from fastapi import FastAPI, Depends, HTTPException, Response, Request
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy.ext.asyncio import AsyncSession
from contextlib import asynccontextmanager
from typing import List, Dict, Union, Optional
import logging
import sys
from datetime import datetime
from fastapi.responses import JSONResponse
from fastapi import status
from fastapi.security import APIKeyHeader
from database import SessionLocal, init_db
from repository import ItemRepository, GameRepository, AnalysisRepository
from models import (
    ItemCreate, ItemUpdate, ItemResponse, GameResponse,
    PlayerPerformanceResponse, OpeningStatsResponse, MoveCountAnalysis
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('api.log')
    ]
)

logger = logging.getLogger(__name__)

# API Key security scheme
API_KEY_HEADER = APIKeyHeader(name="X-API-Key")

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifecycle manager"""
    try:
        await init_db()
        logger.info("Application startup completed")
        yield
    finally:
        logger.info("Application shutdown completed")

# Initialize FastAPI application
app = FastAPI(
    title="Chess Analysis API",
    description="API for chess game analysis and statistics",
    version="1.0.0",
    lifespan=lifespan,
    docs_url="/api/docs",
    redoc_url="/api/redoc",
    openapi_url="/api/openapi.json"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "http://localhost:3000",
        "http://frontend:5173",
        "http://192.168.1.30:5173"
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
    expose_headers=["*"],
    max_age=3600
)

# Database session dependency
async def get_db() -> AsyncSession:
    """Database session manager"""
    db = SessionLocal()
    try:
        yield db
    finally:
        await db.close()

# Error handling
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """Global exception handler"""
    error_id = datetime.utcnow().isoformat()
    logger.error(f"Error ID: {error_id}", exc_info=exc)
    
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "error": "Internal server error",
            "error_id": error_id,
            "message": str(exc),
            "timestamp": datetime.utcnow().isoformat()
        }
    )

# Health check endpoint
@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "service": "chess-backend",
        "version": "1.0.0"
    }

# Item endpoints
@app.post("/items", response_model=ItemResponse)
async def create_item(
    item: ItemCreate,
    db: AsyncSession = Depends(get_db)
):
    """Create new item"""
    repo = ItemRepository(db)
    return await repo.create_item(item)

@app.get("/items", response_model=List[ItemResponse])
async def read_items(db: AsyncSession = Depends(get_db)):
    """Get all items"""
    repo = ItemRepository(db)
    return await repo.get_items()

@app.get("/items/{item_id}", response_model=ItemResponse)
async def read_item(item_id: int, db: AsyncSession = Depends(get_db)):
    """Get specific item"""
    repo = ItemRepository(db)
    item = await repo.get_item(item_id)
    if item is None:
        raise HTTPException(status_code=404, detail="Item not found")
    return item

@app.put("/items/{item_id}", response_model=ItemResponse)
async def update_item(
    item_id: int,
    item: ItemUpdate,
    db: AsyncSession = Depends(get_db)
):
    """Update item"""
    repo = ItemRepository(db)
    updated_item = await repo.update_item(item_id, item)
    if updated_item is None:
        raise HTTPException(status_code=404, detail="Item not found")
    return updated_item

@app.delete("/items/{item_id}")
async def delete_item(item_id: int, db: AsyncSession = Depends(get_db)):
    """Delete item"""
    repo = ItemRepository(db)
    success = await repo.delete_item(item_id)
    if not success:
        raise HTTPException(status_code=404, detail="Item not found")
    return {"ok": True}

# Game endpoints
@app.get("/games", response_model=List[GameResponse])
async def read_games(
    response: Response,
    player_name: Optional[str] = None,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    limit: int = 50,
    offset: int = 0,
    db: AsyncSession = Depends(get_db)
):
    """Get games with filtering"""
    repo = GameRepository(db)
    games = await repo.get_games(
        player_name=player_name,
        start_date=start_date,
        end_date=end_date,
        limit=limit
    )
    response.headers["Cache-Control"] = "public, max-age=300"
    return games

@app.get("/games/{game_id}", response_model=GameResponse)
async def read_game(game_id: int, db: AsyncSession = Depends(get_db)):
    """Get specific game"""
    repo = GameRepository(db)
    game = await repo.get_game(game_id)
    if game is None:
        raise HTTPException(status_code=404, detail="Game not found")
    return game

@app.get("/players/{player_id}/games", response_model=List[GameResponse])
async def read_player_games(player_id: int, db: AsyncSession = Depends(get_db)):
    """Get games for specific player"""
    repo = GameRepository(db)
    games = await repo.get_player_games(player_id)
    if not games:
        raise HTTPException(status_code=404, detail="No games found for this player")
    return games

@app.get("/stats/games")
async def get_game_stats(db: AsyncSession = Depends(get_db)):
    """Get game statistics"""
    repo = GameRepository(db)
    return await repo.get_game_stats()

# Analysis endpoints
@app.get("/analysis/move-counts", response_model=List[MoveCountAnalysis])
async def get_move_count_distribution(
    response: Response,
    db: AsyncSession = Depends(get_db)
):
    """Get move count distribution"""
    try:
        repo = AnalysisRepository(db)
        results = await repo.get_move_count_distribution()
        response.headers["Cache-Control"] = "public, max-age=300"
        return results
    except Exception as e:
        logger.error(f"Error analyzing move counts: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to analyze move counts"
        )

@app.get("/players/{player_id}/performance", response_model=List[PlayerPerformanceResponse])
async def get_player_performance(
    player_id: int,
    time_range: str = 'monthly',
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    response: Response = None,
    db: AsyncSession = Depends(get_db)
):
    """Get player performance statistics"""
    try:
        repo = AnalysisRepository(db)
        performance_data = await repo.get_player_performance_timeline(
            player_id=player_id,
            time_range=time_range,
            start_date=start_date,
            end_date=end_date
        )
        
        if response:
            response.headers["Cache-Control"] = "public, max-age=300"
        
        if not performance_data:
            raise HTTPException(
                status_code=404,
                detail="No performance data found for player"
            )
        
        return performance_data
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/players/{player_id}/openings", response_model=List[OpeningStatsResponse])
async def get_player_openings(
    player_id: int,
    min_games: int = 5,
    response: Response = None,
    db: AsyncSession = Depends(get_db)
):
    """Get player opening statistics"""
    try:
        repo = AnalysisRepository(db)
        opening_stats = await repo.get_player_opening_stats(
            player_id=player_id,
            min_games=min_games
        )
        
        if response:
            response.headers["Cache-Control"] = "public, max-age=300"
        
        if not opening_stats:
            raise HTTPException(
                status_code=404,
                detail="No opening statistics found for player"
            )
        
        return opening_stats
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))




if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        workers=4
    )
### End of ./backend/main.py

### File: ./backend/modules/ops/config.py
### Size: 332 bytes
### Content:
from dataclasses import dataclass
import os

@dataclass
class DatabaseConfig:
    host: str = os.getenv('DB_HOST', 'localhost')
    port: int = int(os.getenv('DB_PORT', 5433))
    database: str = os.getenv('DB_NAME', 'chess')
    user: str = os.getenv('DB_USER', 'postgres')
    password: str = os.getenv('DB_PASSWORD', 'chesspass')
### End of ./backend/modules/ops/config.py

### File: ./backend/modules/ops/performance.py
### Size: 3325 bytes
### Content:
import re
import sys

# Regex patterns for extracting metric fields from the metrics block
metrics_pattern = re.compile(
    r'Elapsed Time:\s+(?P<elapsed_time>\S+)\s+seconds.*?'
    r'Files Processed:\s+(?P<files_processed>\d+).*?'
    r'Files Failed:\s+(?P<files_failed>\d+).*?'
    r'Games Processed:\s+(?P<games_processed>\d+).*?'
    r'Games Failed:\s+(?P<games_failed>\d+).*?'
    r'Database Operations:\s+(?P<db_operations>\d+).*?'
    r'Database Retries:\s+(?P<db_retries>\d+).*?'
    r'Average Processing Speed:\s+(?P<avg_speed>\S+)\sgames/second.*?'
    r'Current Processing Rate:\s+(?P<current_rate>\S+)\sgames/second.*?'
    r'Average File Processing Time:\s+(?P<avg_file_time>\S+)\sseconds.*?'
    r'Success Rate:\s+(?P<success_rate>\S+)%', 
    re.DOTALL
)

def parse_metrics_blocks(log_content):
    # Extract all metric blocks
    blocks = [m.groupdict() for m in metrics_pattern.finditer(log_content)]
    return blocks

def compute_overall_metrics(blocks):
    # Aggregate metrics
    total_files_processed = sum(int(b['files_processed']) for b in blocks)
    total_files_failed = sum(int(b['files_failed']) for b in blocks)
    total_games_processed = sum(int(b['games_processed']) for b in blocks)
    total_db_ops = sum(int(b['db_operations']) for b in blocks)
    total_db_retries = sum(int(b['db_retries']) for b in blocks)

    # Compute averages from all blocks
    avg_processing_speed = sum(float(b['avg_speed']) for b in blocks) / len(blocks)
    avg_current_rate = sum(float(b['current_rate']) for b in blocks) / len(blocks)
    avg_file_processing_time = sum(float(b['avg_file_time']) for b in blocks) / len(blocks)
    avg_success_rate = sum(float(b['success_rate']) for b in blocks) / len(blocks)

    return {
        'total_files_processed': total_files_processed,
        'total_files_failed': total_files_failed,
        'total_games_processed': total_games_processed,
        'total_db_operations': total_db_ops,
        'total_db_retries': total_db_retries,
        'avg_processing_speed': avg_processing_speed,
        'avg_current_rate': avg_current_rate,
        'avg_file_processing_time': avg_file_processing_time,
        'avg_success_rate': avg_success_rate
    }

if __name__ == "__main__":
    # Usage: python parse_metrics.py logfile.log
    logfile = sys.argv[1]

    with open(logfile, 'r', encoding='utf-8') as f:
        content = f.read()

    blocks = parse_metrics_blocks(content)
    if not blocks:
        print("No Pipeline Metrics blocks found.")
        sys.exit(0)

    overall = compute_overall_metrics(blocks)

    print("Overall Metrics:")
    print("----------------")
    print(f"Total Files Processed: {overall['total_files_processed']}")
    print(f"Total Files Failed: {overall['total_files_failed']}")
    print(f"Total Games Processed: {overall['total_games_processed']}")
    print(f"Total Database Operations: {overall['total_db_operations']}")
    print(f"Total Database Retries: {overall['total_db_retries']}")
    print(f"Average Processing Speed: {overall['avg_processing_speed']:.2f} games/second")
    print(f"Average Current Rate: {overall['avg_current_rate']:.2f} games/second")
    print(f"Average File Processing Time: {overall['avg_file_processing_time']:.2f} seconds")
    print(f"Average Success Rate: {overall['avg_success_rate']:.2f}%")

### End of ./backend/modules/ops/performance.py

### File: ./backend/modules/ops/opening_pipeline.py
### Size: 11045 bytes
### Content:
import asyncio
import logging
import sys
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import chess.pgn
import io
import asyncpg
import uuid, json
from datetime import datetime
from encode import ChessGameEncoder
import aiofiles

@dataclass
class ChessGameMetadata:
    white_player_id: int
    black_player_id: int
    white_elo: int
    black_elo: int
    date: Optional[datetime]
    result: str
    eco: str
    moves: str

@dataclass
class DatabaseConfig:
    host: str
    port: int
    database: str
    user: str
    password: str

    def get_dsn(self) -> str:
        return f"postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.database}"


class OpeningProcessor:
    def __init__(self, db_config: DatabaseConfig):
        self.db_config = db_config
        self.logger = self._setup_logger()
        self.encoder = ChessGameEncoder()
        self.pool = None
        self.batch_id = str(uuid.uuid4())
        self.stats = {
            'processed': 0,
            'failed': 0,
            'invalid_moves': 0,
            'db_errors': 0
        }

    
    async def initialize_database(self):
        """Initialize database connection and create necessary tables."""
        try:
            self.pool = await asyncpg.create_pool(
                self.db_config.get_dsn(),
                min_size=3,
                max_size=10,
                command_timeout=60
            )
            
            async with self.pool.acquire() as conn:
                await conn.execute('''
                    CREATE TABLE IF NOT EXISTS openings (
                        id SERIAL PRIMARY KEY,
                        name TEXT NOT NULL,
                        moves BYTEA NOT NULL,
                        CONSTRAINT unique_opening_pattern UNIQUE (name, moves)
                    );
                    
                    CREATE INDEX IF NOT EXISTS idx_openings_name ON openings(name);
                ''')
            
            self.logger.info("Database initialized successfully")
            
        except Exception as e:
            self.logger.error(f"Database initialization failed: {str(e)}")
            raise

    async def process_batch(self, lines: List[str]):
        """Process a batch of opening lines for storage."""
        if not lines:
            return
            
        processed_openings = []
        for line in lines:
            try:
                parsed = self.parse_opening_line(line)
                if not parsed:
                    self.stats['invalid_moves'] += 1
                    continue
                    
                eco, name, moves = parsed
                uci_moves = self.convert_to_uci(moves)
                
                if not uci_moves:
                    self.stats['invalid_moves'] += 1
                    continue
                
                game_metadata = ChessGameMetadata(
                    white_player_id=0,
                    black_player_id=0,
                    white_elo=0,
                    black_elo=0,
                    date=None,
                    result='*',
                    eco=eco,
                    moves=uci_moves
                )
                
                moves = self.encoder.encode_game(game_metadata)
                processed_openings.append((name, moves))
                self.stats['processed'] += 1
                
            except Exception as e:
                self.stats['failed'] += 1
                self.logger.error(f"Error processing line: {str(e)}")
                continue
        
        if processed_openings:
            await self.store_batch(processed_openings)

    async def store_batch(self, openings: List[Tuple]):
        """Store a batch of processed openings in the database."""
        if not openings:
            return
            
        async with self.pool.acquire() as conn:
            try:
                async with conn.transaction():
                    await conn.executemany('''
                        INSERT INTO openings (name, moves)
                        VALUES ($1, $2)
                        ON CONFLICT (name, moves) DO NOTHING
                    ''', openings)
                    
            except Exception as e:
                self.stats['db_errors'] += 1
                self.logger.error(f"Database error: {str(e)}")
                raise
    
    def _setup_logger(self) -> logging.Logger:
        logger = logging.getLogger("OpeningProcessor")
        logger.setLevel(logging.INFO)
        
        console_handler = logging.StreamHandler()
        console_format = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        console_handler.setFormatter(console_format)
        logger.addHandler(console_handler)
        return logger
    
    def parse_opening_line(self, line: str) -> Optional[Tuple[str, str, str]]:
        """
        Parse a single line from the TSV file containing chess openings.
        Validates the ECO code, name, and move sequence.
        """
        try:
            parts = line.strip().split('\t')
            if len(parts) != 3:
                raise ValueError(f"Invalid line format: expected 3 parts, got {len(parts)}")
            
            eco, name, moves = parts
            
            # Validate ECO code
            if not eco or not eco.isalnum() or len(eco) != 3:
                raise ValueError(f"Invalid ECO code: {eco}")
            
            # Validate name
            if not name or len(name.strip()) == 0:
                raise ValueError("Empty opening name")
            
            # Validate moves
            if not moves or not any(c.isalpha() for c in moves):
                raise ValueError("Invalid move sequence")
            
            return eco, name.strip(), moves.strip()
            
        except Exception as e:
            self.logger.warning(
                f"Error parsing line: {str(e)}\n"
                f"Original line: {line}"
            )
            return None

    def convert_to_uci(self, moves_text: str) -> str:
        """
        Convert traditional chess notation to UCI format.
        Handles standard chess notation with move numbers (e.g., '1. e4 e5 2. Nf3 Nc6').
        """
        try:
            # Clean up the move text
            cleaned_text = moves_text.strip()
            
            # Remove move numbers and their dots more carefully
            move_parts = []
            for part in cleaned_text.split():
                # Skip the move numbers (parts ending with '.')
                if part.endswith('.'):
                    continue
                # Add the actual move
                move_parts.append(part)
            
            # Join moves back together
            moves_only = ' '.join(move_parts)
            
            # Set up a new game and board
            board = chess.Board()
            uci_moves = []
            
            # Process each move
            for move_san in moves_only.split():
                if move_san.strip():
                    try:
                        move = board.parse_san(move_san)
                        uci_moves.append(move.uci())
                        board.push(move)
                    except ValueError as e:
                        self.logger.warning(
                            f"Invalid move {move_san} in sequence {moves_text}\n"
                            f"Error: {str(e)}"
                        )
                        break
            
            if not uci_moves:
                return ""
                
            return ' '.join(uci_moves)
            
        except Exception as e:
            self.logger.error(
                f"Move conversion error: {str(e)}\n"
                f"Original sequence: {moves_text}"
            )
            return ""
    
    async def process_file(self, file_path: Path) -> bool:
        """Process a single TSV file containing chess openings."""
        try:
            self.logger.info(f"Processing file: {file_path}")
            
            async with aiofiles.open(file_path, mode='r', encoding='utf-8') as f:
                content = await f.read()
            
            lines = content.strip().split('\n')
            if not lines:
                self.logger.warning(f"Empty file: {file_path}")
                return False
                
            # Skip header if present
            if lines[0].lower().startswith('eco') or '\t' not in lines[0]:
                lines = lines[1:]
            
            batch_size = 100
            for i in range(0, len(lines), batch_size):
                batch = lines[i:i + batch_size]
                await self.process_batch(batch)
                
                # Log progress
                total_processed = self.stats['processed'] + self.stats['failed']
                if total_processed > 0:
                    success_rate = (self.stats['processed'] / total_processed) * 100
                    self.logger.info(
                        f"Progress: {i + len(batch)}/{len(lines)} lines processed. "
                        f"Success rate: {success_rate:.2f}%"
                    )
            
            return True
            
        except Exception as e:
            self.logger.error(f"Error processing file {file_path}: {str(e)}")
            return False


    async def cleanup(self):
        """Cleanup resources and log final statistics."""
        if self.pool:
            await self.pool.close()
        
        self.logger.info(
            f"\nProcessing completed:"
            f"\n- Processed successfully: {self.stats['processed']}"
            f"\n- Invalid moves: {self.stats['invalid_moves']}"
            f"\n- Processing failures: {self.stats['failed']}"
            f"\n- Database errors: {self.stats['db_errors']}"
        )

async def main():
    # Create proper DatabaseConfig object instead of dictionary
    db_config = DatabaseConfig(
        host="localhost",
        port=5433,
        database="chess",
        user="postgres",
        password="chesspass"
    )
    
    # Initialize processor
    processor = OpeningProcessor(db_config)
    
    try:
        # Initialize database
        await processor.initialize_database()
        
        # Process each TSV file
        data_dir = Path("data")
        tsv_files = list(data_dir.glob("*.tsv"))
        
        if not tsv_files:
            processor.logger.error("No TSV files found in the data directory")
            return
        
        for file_path in tsv_files:
            success = await processor.process_file(file_path)
            if not success:
                processor.logger.warning(f"Failed to process {file_path}")
                
    except Exception as e:
        processor.logger.error(f"Fatal error: {str(e)}")
        raise
        
    finally:
        await processor.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
### End of ./backend/modules/ops/opening_pipeline.py

### File: ./backend/modules/ops/encode.py
### Size: 7577 bytes
### Content:
from typing import List, Optional, Dict
import chess
import struct
import bitarray
from dataclasses import dataclass

@dataclass
class EncodedMoves:
    """Container for encoded chess moves with metadata"""
    move_count: int  # Total number of moves
    raw_bytes: bytes  # Binary encoded moves
    size_bytes: int  # Size of encoded data

class ChessMoveEncoder:
    """
    Efficient chess move encoder that focuses solely on move encoding.
    Converts UCI moves to a compact binary format for storage.
    """
    def __init__(self):
        # Cache for frequently used move encodings
        self._move_cache: Dict[str, int] = {}
        self._reverse_cache: Dict[int, str] = {}

    def _encode_single_move(self, uci_move: str) -> int:
        """
        Encode a single UCI move into a 16-bit integer.
        
        Format:
        - From square: 6 bits (0-63)
        - To square: 6 bits (0-63)
        - Promotion piece: 4 bits (0-15, where 0 means no promotion)
        
        Args:
            uci_move: Move in UCI format (e.g., 'e2e4', 'd7d8q')
            
        Returns:
            16-bit integer encoding of the move
            
        Raises:
            ValueError: If move format is invalid
        """
        if uci_move in self._move_cache:
            return self._move_cache[uci_move]

        if not (4 <= len(uci_move) <= 5):
            raise ValueError(f"Invalid UCI move format: {uci_move}")

        try:
            from_square = chess.SQUARE_NAMES.index(uci_move[:2])
            to_square = chess.SQUARE_NAMES.index(uci_move[2:4])
        except ValueError as e:
            raise ValueError(f"Invalid square name in move {uci_move}") from e

        promotion = 0
        if len(uci_move) == 5:
            try:
                # Map promotion pieces: p(1), n(2), b(3), r(4), q(5), k(6)
                promotion = "pnbrqk".index(uci_move[4].lower()) + 1
            except ValueError as e:
                raise ValueError(f"Invalid promotion piece in move {uci_move}") from e

        # Combine bits: from_square (6) | to_square (6) | promotion (4)
        encoded = (from_square << 10) | (to_square << 4) | promotion

        # Cache the encoding
        self._move_cache[uci_move] = encoded
        self._reverse_cache[encoded] = uci_move

        return encoded

    def _decode_single_move(self, encoded_move: int) -> str:
        """
        Decode a 16-bit integer back into a UCI move.
        
        Args:
            encoded_move: 16-bit integer representing the encoded move
            
        Returns:
            Move in UCI format
            
        Raises:
            ValueError: If encoded move is invalid
        """
        if encoded_move in self._reverse_cache:
            return self._reverse_cache[encoded_move]

        if not (0 <= encoded_move < 65536):  # 2^16
            raise ValueError(f"Invalid encoded move value: {encoded_move}")

        from_square = (encoded_move >> 10) & 0x3F
        to_square = (encoded_move >> 4) & 0x3F
        promotion = encoded_move & 0xF

        if from_square >= 64 or to_square >= 64:
            raise ValueError(f"Invalid square index in encoded move: {encoded_move}")

        move = chess.SQUARE_NAMES[from_square] + chess.SQUARE_NAMES[to_square]
        if promotion:
            if promotion > 6:
                raise ValueError(f"Invalid promotion value in encoded move: {encoded_move}")
            move += "pnbrqk"[promotion - 1]

        self._reverse_cache[encoded_move] = move
        return move

    def encode_moves(self, moves: List[str]) -> EncodedMoves:
        """
        Encode a list of UCI moves into a compact binary format.
        
        Format:
        - Move count: 16 bits
        - Moves: sequence of 16-bit integers
        
        Args:
            moves: List of moves in UCI format
            
        Returns:
            EncodedMoves object containing the encoded data
            
        Raises:
            ValueError: If any move is invalid
        """
        bits = bitarray.bitarray()

        # Store move count (16 bits)
        bits.frombytes(struct.pack('>H', len(moves)))

        # Encode each move
        for move in moves:
            try:
                encoded_move = self._encode_single_move(move)
                bits.frombytes(struct.pack('>H', encoded_move))
            except ValueError as e:
                raise ValueError(f"Failed to encode move {move}: {str(e)}") from e

        encoded_bytes = bits.tobytes()
        return encoded_bytes
        return EncodedMoves(
            move_count=len(moves),
            raw_bytes=encoded_bytes,
            size_bytes=len(encoded_bytes)
        )

    def decode_moves(self, encoded_data: bytes) -> List[str]:
        """
        Decode binary data back into a list of UCI moves.
        
        Args:
            encoded_data: Binary encoded moves
            
        Returns:
            List of moves in UCI format
            
        Raises:
            ValueError: If data format is invalid
        """
        if len(encoded_data) < 2:  # Minimum size for move count
            raise ValueError("Encoded data too short")

        bits = bitarray.bitarray()
        bits.frombytes(encoded_data)

        try:
            # Read move count
            move_count = struct.unpack('>H', bits[0:16].tobytes())[0]
            offset = 16

            # Validate expected data length
            expected_bits = 16 + (move_count * 16)
            if len(bits) < expected_bits:
                raise ValueError(
                    f"Encoded data too short for {move_count} moves. "
                    f"Expected {expected_bits} bits, got {len(bits)}"
                )

            # Decode moves
            moves = []
            for _ in range(move_count):
                encoded_move = struct.unpack(
                    '>H', 
                    bits[offset:offset + 16].tobytes()
                )[0]
                moves.append(self._decode_single_move(encoded_move))
                offset += 16

            return moves

        except (struct.error, ValueError) as e:
            raise ValueError(f"Failed to decode moves: {str(e)}") from e

    def validate_moves(self, moves: List[str]) -> bool:
        """
        Validate that all moves are in correct UCI format and could be legally encoded.
        
        Args:
            moves: List of moves to validate
            
        Returns:
            True if all moves are valid, False otherwise
        """
        try:
            board = chess.Board()
            for move in moves:
                if not (4 <= len(move) <= 5):
                    return False
                    
                # Validate square names
                from_square = move[:2]
                to_square = move[2:4]
                if from_square not in chess.SQUARE_NAMES or to_square not in chess.SQUARE_NAMES:
                    return False
                    
                # Validate promotion if present
                if len(move) == 5 and move[4].lower() not in 'pnbrqk':
                    return False
                    
                # Validate move is legal
                try:
                    chess_move = chess.Move.from_uci(move)
                    if not chess_move in board.legal_moves:
                        return False
                    board.push(chess_move)
                except ValueError:
                    return False
                    
            return True
            
        except Exception:
            return False
### End of ./backend/modules/ops/encode.py

### File: ./backend/modules/ops/game_pipeline.py
### Size: 29492 bytes
### Content:
import asyncio
import aiohttp
import asyncpg
import chess.pgn
from pathlib import Path
from typing import Optional, List, Dict, Tuple
from dataclasses import dataclass
import logging
import re
import tempfile
import io
from bs4 import BeautifulSoup
import zipfile
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor
import multiprocessing as mp
from tqdm.asyncio import tqdm
import time
import aiofiles, os
from urllib.parse import urljoin
import struct
import chess
import sys
import bitarray
from typing import Optional, Tuple, List
from encode import ChessMoveEncoder
class TemporaryDirectory:
    def __init__(self, prefix=None):
        self.prefix = prefix
        self.path = None

    async def __aenter__(self):
        self.path = Path(tempfile.mkdtemp(prefix=self.prefix))
        return self.path

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.path and self.path.exists():
            for dirpath, _, filenames in os.walk(self.path, topdown=False):
                for filename in filenames:
                    try:
                        os.remove(os.path.join(dirpath, filename))
                    except:
                        pass
                try:
                    os.rmdir(dirpath)
                except:
                    pass

@dataclass
class DatabaseConfig:
    host: str
    port: int
    database: str
    user: str
    password: str

    def get_dsn(self) -> str:
        return f"postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.database}"

@dataclass
class ProcessingConfig:
    max_open_files: int = 5
    db_batch_size: int = 1000
    parsing_chunk_size: int = 50_000
    download_concurrency: int = 1
    process_pool_size: Optional[int] = None
    progress_update_interval: float = 0.5

    def __post_init__(self):
        if self.process_pool_size is None:
            self.process_pool_size = max(1, mp.cpu_count() - 1)

def parse_pgn_chunk(chunk: str) -> List[Dict]:
    games = []
    pgn = io.StringIO(chunk)
    while True:
        try:
            game = chess.pgn.read_game(pgn)
            if game is None:
                break
            headers = game.headers
            moves = [move.uci() for move in game.mainline_moves()]
            games.append({
                'white': headers.get('White', 'Unknown'),
                'black': headers.get('Black', 'Unknown'),
                'white_elo': int(headers.get("WhiteElo", "0") or 0),
                'black_elo': int(headers.get("BlackElo", "0") or 0),
                'date': headers.get('Date', ''),
                'result': headers.get('Result', '*'),
                'eco': headers.get('ECO', 'A00'),  # default if missing
                'moves': moves
            })
        except:
            continue
    return games

@dataclass
class ChessGameMetadata:
    white_player_id: int
    black_player_id: int
    white_elo: int
    black_elo: int
    date: Optional[datetime]
    result: str
    eco: str
    moves: str



class PipelineMetrics:
    def __init__(self):
        self.start_time = time.time()
        self.files_processed = 0
        self.files_failed = 0
        self.games_processed = 0
        self.games_failed = 0
        self.db_operations = 0
        self.db_retries = 0
        self.processing_times = []
        self.current_rate = 0

    def log_metrics(self, logger):
        elapsed = time.time() - self.start_time
        avg_speed = self.games_processed / elapsed if elapsed > 0 else 0
        avg_time = sum(self.processing_times) / max(len(self.processing_times), 1)
        success_rate = (self.games_processed / (self.games_processed + self.games_failed) * 100) if (self.games_processed + self.games_failed) > 0 else 0
        logger.info(
            f"\nPipeline Metrics:"
            f"\n----------------"
            f"\nElapsed Time: {elapsed:.2f} seconds"
            f"\nFiles Processed: {self.files_processed}"
            f"\nFiles Failed: {self.files_failed}"
            f"\nGames Processed: {self.games_processed}"
            f"\nGames Failed: {self.games_failed}"
            f"\nDatabase Operations: {self.db_operations}"
            f"\nDatabase Retries: {self.db_retries}"
            f"\nAverage Processing Speed: {avg_speed:.2f} games/second"
            f"\nCurrent Processing Rate: {self.current_rate:.2f} games/second"
            f"\nAverage File Processing Time: {avg_time:.2f} seconds"
            f"\nSuccess Rate: {success_rate:.2f}%"
        )

    def display_in_place_metrics(self):
        elapsed = time.time() - self.start_time
        avg_speed = self.games_processed / elapsed if elapsed > 0 else 0
        success_rate = (self.games_processed / (self.games_processed + self.games_failed) * 100) if (self.games_processed + self.games_failed) > 0 else 0
        # Construct a concise, single-line display
        # Use \r to return to the start of the line and overwrite in place
        metrics_line = (
            f"\rFiles: {self.files_processed} | "
            f"Games: {self.games_processed} processed/{self.games_failed} failed | "
            f"Speed: {avg_speed:.2f} g/s | "
            f"Success: {success_rate:.2f}%"
        )
        # Print without adding a newline, and flush
        sys.stdout.write(metrics_line)
        sys.stdout.flush()

class ChessDataPipeline:
    def __init__(self, db_config: DatabaseConfig, processing_config: ProcessingConfig):
        self.db_config = db_config
        self.config = processing_config
        self.db_pool = None
        self.process_pool = ProcessPoolExecutor(max_workers=self.config.process_pool_size)
        self.download_dir = Path(tempfile.mkdtemp())
        self.base_url = "https://www.pgnmentor.com"
        self.logger = self._setup_logger()
        self.metrics = PipelineMetrics()

        self.file_semaphore = asyncio.Semaphore(self.config.max_open_files)
        self.file_lock = asyncio.Lock()
        self.download_sem = asyncio.Semaphore(self.config.download_concurrency)
        self.http_session = None

        self.processed_files = set()
        self.encoder = ChessMoveEncoder()

    def _setup_logger(self) -> logging.Logger:
        logger = logging.getLogger("ChessPipeline")
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        return logger

    def log_metrics(self):
        self.metrics.display_in_place_metrics()

    async def initialize(self):
        self.db_pool = await asyncpg.create_pool(
            self.db_config.get_dsn(),
            min_size=3,
            max_size=10,
            command_timeout=60
        )
        async with self.db_pool.acquire() as conn:
            await conn.execute('''
                CREATE TABLE IF NOT EXISTS players (
                    id SERIAL PRIMARY KEY,
                    name TEXT NOT NULL,
                    CONSTRAINT unique_player_name UNIQUE (name)
                );
                CREATE TABLE IF NOT EXISTS games (
                    id SERIAL PRIMARY KEY,
                    white_player_id INTEGER REFERENCES players(id),
                    black_player_id INTEGER REFERENCES players(id),
                    white_elo INTEGER,
                    black_elo INTEGER,
                    date DATE,
                    result VARCHAR(10),
                    eco VARCHAR(10),
                    moves BYTEA,
                    CONSTRAINT valid_result CHECK (result IN ('1-0', '0-1', '1/2-1/2', '*')),
                    CONSTRAINT unique_game UNIQUE (white_player_id, black_player_id, date, moves)
                );
                CREATE INDEX IF NOT EXISTS idx_players_name ON players(name);
                CREATE INDEX IF NOT EXISTS idx_games_players ON games(white_player_id, black_player_id);
            ''')

    async def get_pgn_links(
        self,
        start_id: Optional[int] = None,
        end_id: Optional[int] = None
    ) -> List[Dict]:
        """
        Collects `.zip` or `.pgn` files from the 'players', 'openings', and 'events' directories.
        """
        target_url = urljoin(self.base_url, "files.html")
        async with self.http_session.get(target_url) as response:
            if response.status != 200:
                raise Exception(f"Failed to fetch PGN links: {response.status}")

            content = await response.text()
            soup = BeautifulSoup(content, 'html.parser')

            # Now, we also target 'openings/' and 'events/' directories in addition to 'players/'.
            link_candidates = soup.select(
                'a[href^="players/"], a[href^="openings/"], a[href^="events/"]'
            )

            valid_extensions = ('.zip', '.pgn')
            all_links = []
            for a in link_candidates:
                href = a.get('href', '').strip()
                if href.lower().endswith(valid_extensions):
                    full_url = urljoin(self.base_url, href)
                    all_links.append(full_url)

            unique_links = sorted(set(all_links))

            selected_links = []
            for idx, link in enumerate(unique_links):
                if (start_id is None or idx >= start_id) and (end_id is None or idx <= end_id):
                    selected_links.append({
                        'url': link,
                        'filename': Path(link).name,
                        'id': idx
                    })

            self.logger.info(f"Found {len(selected_links)} PGN files from players, openings, and events directories in selected range")
            return selected_links


    async def download_file(self, url: str, filename: str, pbar: tqdm) -> Optional[Path]:
        filepath = self.download_dir / filename
        async with self.download_sem:
            try:
                async with self.http_session.get(url) as response:
                    if response.status != 200:
                        raise Exception(f"Download failed with status: {response.status}")
                    total_size = int(response.headers.get('content-length', 0))
                    if total_size == 0:
                        raise Exception("Content length is zero")
                    pbar.total = total_size
                    downloaded_size = 0
                    async with aiofiles.open(filepath, 'wb') as f:
                        async for chunk in response.content.iter_chunked(8192):
                            if chunk:
                                await f.write(chunk)
                                downloaded_size += len(chunk)
                                pbar.update(len(chunk))
                    if downloaded_size != total_size:
                        raise Exception(f"Incomplete download: {downloaded_size}/{total_size}")
                    if not filepath.exists() or filepath.stat().st_size != total_size:
                        raise Exception("File size verification failed")
                    return filepath
            except Exception as e:
                self.logger.error(f"Download error for {filename}: {str(e)}")
                if filepath.exists():
                    try:
                        os.remove(filepath)
                    except:
                        pass
                return None

    async def extract_zip(self, zip_path: Path) -> List[Path]:
        if not zip_path.exists():
            pass
            # self.logger.error(f"ZIP file not found: {zip_path}")
            return []
        extract_dir = self.download_dir / 'extracted'
        extract_dir.mkdir(exist_ok=True)
        def _extract():
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(extract_dir)
            return list(extract_dir.glob('**/*.pgn'))
        loop = asyncio.get_event_loop()
        try:
            pgn_files = await loop.run_in_executor(None, _extract)
            pgn_files = [p for p in pgn_files if p.exists()]    
            return pgn_files
        except Exception as e:
            # self.logger.error(f"Error extracting {zip_path}: {str(e)}")
            return []
        finally:
            if zip_path.exists():
                try:
                    os.remove(zip_path)
                except Exception as e:
                    pass
                    # self.logger.error(f"Error removing zip file {zip_path}: {str(e)}")

    async def store_games_batch(self, games: List[Dict], games_pbar: Optional[tqdm] = None):
        if not games:
            return
        batch_start = time.time()
        max_retries = 5
        base_delay = 0.1
        async with self.db_pool.acquire() as conn:
            unique_players = sorted({g['white'] for g in games} | {g['black'] for g in games})
            player_ids = {}
            # Insert players
            for player_name in unique_players:
                for attempt in range(max_retries):
                    try:
                        self.metrics.db_operations += 1
                        player_id = await conn.fetchval('''
                            INSERT INTO players (name)
                            VALUES ($1)
                            ON CONFLICT (name) DO UPDATE 
                            SET name = EXCLUDED.name
                            RETURNING id
                        ''', player_name)
                        player_ids[player_name] = player_id
                        break
                    except Exception as e:
                        self.metrics.db_retries += 1
                        if attempt == max_retries - 1:
                            self.logger.error(f"Failed to process player {player_name}: {str(e)}")
                            # If a player fails entirely, skip their games (very rare scenario)
                            # but we won't raise to avoid halting the entire pipeline.
                            # Filter out games involving this player
                            games = [g for g in games if g['white'] != player_name and g['black'] != player_name]
                        await asyncio.sleep(base_delay * (2 ** attempt))

            sub_batch_size = 50
            successful_games = 0
            failed_games = 0

            for i in range(0, len(games), sub_batch_size):
                sub_batch = games[i:i + sub_batch_size]
                records = []
                for g in sub_batch:
                    try:
                        date_parsed = self._parse_date(g['date'])
                        # Validate and sanitize ECO (if invalid, skip the game)
                        eco = g['eco']
                        if not (eco and len(eco) >= 3 and eco[0].isalpha() and eco[1:].isdigit()):
                            raise ValueError(f"Invalid ECO code: {eco}")
                        
                        # Validate result
                        if g['result'] not in ['1-0', '0-1', '1/2-1/2', '*']:
                            raise ValueError(f"Invalid result: {g['result']}")

                        metadata = ChessGameMetadata(
                            white_player_id=player_ids[g['white']],
                            black_player_id=player_ids[g['black']],
                            white_elo=g['white_elo'],
                            black_elo=g['black_elo'],
                            date=date_parsed,
                            result=g['result'],
                            eco=eco,
                            moves=g['moves']
                        )

                        encoded_game = self.encoder.encode_moves(metadata.moves)

                        records.append((
                            metadata.white_player_id,
                            metadata.black_player_id,
                            metadata.white_elo,
                            metadata.black_elo,
                            metadata.date,
                            metadata.result,
                            metadata.eco,
                            encoded_game
                        ))
                    except Exception as e:
                        # If metadata preparation fails, skip this game
                        failed_games += 1
                        self.metrics.games_failed += 1
                        self.logger.error(f"Skipping invalid game data: {str(e)}")

                if not records:
                    continue

                # Attempt a batch insert
                for attempt in range(max_retries):
                    try:
                        self.metrics.db_operations += 1
                        async with conn.transaction(isolation='read_committed'):
                            await conn.executemany('''
                                INSERT INTO games (
                                    white_player_id, black_player_id, white_elo, black_elo, date, result, eco, moves
                                ) VALUES (
                                    $1, $2, $3, $4, $5, $6, $7, $8::bytea
                                )
                            ''', records)
                        # If successful, update counts and move on
                        successful_games += len(records)
                        if games_pbar:
                            games_pbar.update(len(records))
                        break
                    except Exception as e:
                        self.metrics.db_retries += 1
                        if attempt == max_retries - 1:
                            # If the whole sub-batch fails, fallback to individual insertion
                            self.logger.error(f"Batch insert failed, attempting individual inserts: {str(e)}")
                            # Insert one by one
                            insert_success = 0
                            for r in records:
                                try:
                                    async with conn.transaction(isolation='read_committed'):
                                        await conn.execute('''
                                            INSERT INTO games (
                                                white_player_id, black_player_id, white_elo, black_elo, date, result, eco, moves
                                            ) VALUES (
                                                $1, $2, $3, $4, $5, $6, $7, $8::bytea
                                            )
                                        ''', *r)
                                    insert_success += 1
                                    if games_pbar:
                                        games_pbar.update(1)
                                except Exception as ind_e:
                                    # If individual game fails, skip it
                                    failed_games += 1
                                    self.metrics.games_failed += 1
                                    self.logger.error(f"Skipping problematic game: {str(ind_e)}")

                            successful_games += insert_success
                        else:
                            await asyncio.sleep(base_delay * (2 ** attempt))

            batch_time = time.time() - batch_start
            self.metrics.current_rate = successful_games / batch_time if batch_time > 0 else 0
            self.metrics.games_processed += successful_games

            # We don't raise exceptions here, ensuring the pipeline continues.
            # Failed games have been counted and logged. 


    def _split_pgn_content(self, content: str) -> List[str]:
        chunks = []
        current_chunk = []
        current_size = 0
        for line in content.splitlines():
            if line.startswith('[Event "') and current_size >= self.config.parsing_chunk_size:
                chunks.append('\n'.join(current_chunk))
                current_chunk = []
                current_size = 0
            current_chunk.append(line)
            current_size += 1
        if current_chunk:
            chunks.append('\n'.join(current_chunk))
        return chunks

    def _parse_date(self, date_str: str) -> Optional[datetime]:
        if not date_str or date_str == "???":
            return None
        for fmt in ("%Y.%m.%d", "%Y/%m/%d", "%Y-%m-%d"):
            try:
                return datetime.strptime(date_str, fmt).date()
            except ValueError:
                pass
        year_only = date_str.replace(".", "")
        if len(year_only) == 4 and year_only.isdigit():
            return datetime(int(year_only), 1, 1).date()
        return None

    async def process_pgn_file(self, file_path: Path, games_pbar: Optional[tqdm] = None) -> Tuple[int, int]:
        async with self.file_semaphore:
            file_start = time.time()
            if not file_path.exists():
                if file_path not in self.processed_files:
                    # self.logger.error(f"Skipping non-existent file: {file_path}")
                    self.processed_files.add(file_path)
                    self.metrics.files_failed += 1
                return 0, 0

            try:
                try:
                    async with aiofiles.open(file_path, encoding='utf-8') as f:
                        content = await f.read()
                except UnicodeDecodeError:
                    async with aiofiles.open(file_path, encoding='latin-1') as f:
                        content = await f.read()

                if not content:
                    return 0, 0

                total_games = len(list(re.finditer(r'\[Event\s+"[^"]*"\s*\]', content)))
                if games_pbar is not None:
                    games_pbar.total = total_games
                    games_pbar.refresh()

                chunks = self._split_pgn_content(content)

                async def parse_chunk(chunk):
                    loop = asyncio.get_event_loop()
                    return await loop.run_in_executor(self.process_pool, parse_pgn_chunk, chunk)

                parsed_results = await asyncio.gather(*[parse_chunk(ch) for ch in chunks])
                all_games = [g for lst in parsed_results for g in lst]

                batch_size = min(50, self.config.db_batch_size)
                processed_count = 0
                for i in range(0, len(all_games), batch_size):
                    batch = all_games[i:i + batch_size]
                    await self.store_games_batch(batch, games_pbar)
                    processed_count += len(batch)

                file_time = time.time() - file_start
                self.metrics.processing_times.append(file_time)
                self.metrics.files_processed += 1

                failed_games = max(0, total_games - processed_count)
                self.metrics.games_failed += failed_games
                return processed_count, failed_games

            except FileNotFoundError:
                if file_path not in self.processed_files:
                    # self.logger.error(f"File not found during processing: {file_path}")
                    self.processed_files.add(file_path)
                self.metrics.files_failed += 1
                return 0, 0
            except Exception as e:
                # self.logger.error(f"Error processing {file_path}: {str(e)}")
                self.metrics.files_failed += 1
                return 0, 0
            finally:
                if file_path.exists():
                    try:
                        os.remove(file_path)
                    except Exception as e:
                        pass
                        # self.logger.error(f"Error removing file {file_path}: {str(e)}")

    async def process_all(self):
        async with TemporaryDirectory(prefix='chess_') as temp_dir:
            self.download_dir = temp_dir
            self.http_session = aiohttp.ClientSession()
            try:
                links = await self.get_pgn_links()
                if not links:
                    self.logger.error("No PGN files found")
                    return

                main_pbar = tqdm(total=len(links), desc="Files", position=0, leave=True, unit="file")
                games_pbar = tqdm(desc="Games", position=1, leave=True, unit="game")

                async def process_file(link):
                    download_pbar = tqdm(
                        desc=f"Downloading {link['filename']}",
                        position=2,
                        leave=False,
                        unit='B',
                        unit_scale=True,
                        unit_divisor=1024
                    )
                    filepath = await self.download_file(link['url'], link['filename'], download_pbar)
                    download_pbar.close()

                    if not filepath:
                        self.logger.error(f"Failed to download {link['filename']}")
                        main_pbar.update(1)
                        return 0, 1

                    if filepath.suffix == '.zip':
                        pgn_files = await self.extract_zip(filepath)
                        if not pgn_files:
                            # self.logger.error(f"No PGN files found in {link['filename']}")
                            main_pbar.update(1)
                            return 0, 1

                        processed_sum = 0
                        failed_sum = 0
                        for pgn_path in pgn_files:
                            processed, failed = await self.process_pgn_file(pgn_path, games_pbar)
                            processed_sum += processed
                            failed_sum += failed
                        main_pbar.update(1)
                        if processed_sum > 0:
                            self.log_inplace_metrics()
                        return processed_sum, failed_sum
                    else:
                        processed, failed = await self.process_pgn_file(filepath, games_pbar)
                        main_pbar.update(1)
                        if processed > 0:
                            self.log_metrics()
                        return processed, failed

                tasks = [process_file(link) for link in links]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                total_processed = 0
                total_failed = 0
                for result in results:
                    if isinstance(result, tuple):
                        p, f = result
                        total_processed += p
                        total_failed += f
                    else:
                        self.logger.error(f"Task failed with exception: {str(result)}")
                        total_failed += 1

                main_pbar.close()
                games_pbar.close()

                success_rate = (total_processed/(total_processed + total_failed)*100) if (total_processed + total_failed) > 0 else 0
                self.logger.info(
                    f"\nProcessing Summary:"
                    f"\n------------------"
                    f"\nTotal files attempted: {len(links)}"
                    f"\nTotal games processed: {total_processed}"
                    f"\nTotal games failed: {total_failed}"
                    f"\nSuccess rate: {success_rate:.2f}%"
                )
                self.metrics.log_metrics()

            except Exception as e:
                self.logger.error(f"Fatal error in pipeline: {str(e)}")
                raise
            finally:
                await self.cleanup()
                await self.http_session.close()

    async def cleanup(self):
        try:
            if self.process_pool:
                self.process_pool.shutdown(wait=True)
            if self.db_pool:
                await self.db_pool.close()

            async with self.file_lock:
                pass

            if self.download_dir.exists():
                for dirpath, _, filenames in os.walk(self.download_dir, topdown=False):
                    for filename in filenames:
                        try:
                            os.remove(os.path.join(dirpath, filename))
                        except:
                            pass
                    try:
                        os.rmdir(dirpath)
                    except:
                        pass
                try:
                    os.rmdir(self.download_dir)
                except:
                    pass
        except Exception as e:
            self.logger.error(f"Error during cleanup: {str(e)}")

async def main():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('chess_pipeline.log')
        ]
    )

    db_config = DatabaseConfig(
        host="localhost",
        port=5433,
        database="chess",
        user="postgres",
        password="chesspass"
    )

    processing_config = ProcessingConfig(
        db_batch_size=1000,
        parsing_chunk_size=50000,
        download_concurrency=1
    )

    pipeline = ChessDataPipeline(db_config, processing_config)
    try:
        await pipeline.initialize()
        await pipeline.process_all()
    except KeyboardInterrupt:
        print("\nProcess interrupted by user")
    except Exception as e:
        print(f"Fatal error: {str(e)}")
        raise
    finally:
        await pipeline.cleanup()

if __name__ == "__main__":
    asyncio.run(main())

### End of ./backend/modules/ops/game_pipeline.py

### File: ./backend/repository.py
### Size: 32524 bytes
### Content:
from sqlalchemy.orm import Session,joinedload
from sqlalchemy import select, or_
from sqlalchemy.ext.asyncio import AsyncSession
from models import ItemDB, ItemCreate, ItemUpdate
from models import GameDB, PlayerDB 
from datetime import datetime
from typing import List, Optional, Dict, Any
import chess
import bitarray
import struct
from models import GameDB, PlayerDB, GameResponse
import logging

from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.engine import Result
from typing import List, Dict, Union, TypedDict, Optional, Any, Tuple
import logging
from datetime import datetime
from modules.ops.encode import ChessMoveEncoder
class ItemRepository:
    def __init__(self, db: AsyncSession):
        self.db = db

    async def create_item(self, item: ItemCreate) -> ItemDB:
        db_item = ItemDB(**item.model_dump())
        self.db.add(db_item)
        await self.db.commit()
        await self.db.refresh(db_item)
        return db_item

    async def get_items(self) -> list[ItemDB]:
        result = await self.db.execute(select(ItemDB))
        return result.scalars().all()

    async def get_item(self, item_id: int) -> ItemDB | None:
        result = await self.db.execute(select(ItemDB).filter(ItemDB.id == item_id))
        return result.scalar_one_or_none()

    async def update_item(self, item_id: int, item: ItemUpdate) -> ItemDB | None:
        db_item = await self.get_item(item_id)
        if not db_item:
            return None
            
        update_data = item.model_dump(exclude_unset=True)
        for key, value in update_data.items():
            setattr(db_item, key, value)
            
        await self.db.commit()
        await self.db.refresh(db_item)
        return db_item

    async def delete_item(self, item_id: int) -> bool:
        db_item = await self.get_item(item_id)
        if not db_item:
            return False
            
        await self.db.delete(db_item)
        await self.db.commit()
        return True
    

class GameRepository:
    """
    Enhanced repository layer for chess game data with move format conversion.
    """
    
    def __init__(self, db: AsyncSession):
        """
        Initialize repository with database session and decoder.
        
        Args:
            db: Asynchronous database session
        """
        self.db = db
        self.decoder = ChessMoveEncoder()

    async def get_games(
        self,
        player_name: Optional[str] = None,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        limit: int = 50,
        move_notation: str = 'uci'  # Options: 'uci' or 'san'
    ) -> List[GameResponse]:
        """
        Retrieve games with optional filtering and move notation conversion.
        
        Args:
            player_name: Filter by player name (optional)
            start_date: Filter by start date (optional)
            end_date: Filter by end date (optional)
            limit: Maximum number of games to return
            move_notation: Desired move notation format ('uci' or 'san')
            
        Returns:
            List of GameResponse objects with decoded moves
        """
        # Construct base query
        query = (
            select(GameDB)
            .options(
                joinedload(GameDB.white_player),
                joinedload(GameDB.black_player)
            )
        )

        query = query.where(GameDB.date.isnot(None))
        
        # Apply filters
        if player_name:
            player_filter = or_(
                GameDB.white_player.has(
                    PlayerDB.name.ilike(f'%{player_name}%')
                ),
                GameDB.black_player.has(
                    PlayerDB.name.ilike(f'%{player_name}%')
                )
            )
            query = query.where(player_filter)

        if start_date:
            query = query.where(GameDB.date >= start_date)
        
        if end_date:
            query = query.where(GameDB.date <= end_date)

        # Apply limit and ordering
        query = query.order_by(GameDB.date.desc()).limit(limit)

        # Execute query
        result = await self.db.execute(query)
        games = result.scalars().unique().all()
        # Process games and convert moves
        processed_games = []
        counter = 0 
        limit = 1
        for game in games:
            if counter >= 100:
                break
            try:
                # Decode binary game data
                moves = self.decoder.decode_moves(game.moves)
                # Convert moves if needed
                if move_notation == 'san':
                    moves = self.decoder.convert_uci_to_san(moves)

                # Create response object
                processed_game = GameResponse(
                    id=game.id,
                    white_player_id=game.white_player_id,
                    black_player_id=game.black_player_id,
                    white_player=game.white_player,
                    black_player=game.black_player,
                    date=game.date,
                    result=game.result,
                    eco=game.eco,
                    moves=' '.join(moves)
                )
                processed_games.append(processed_game)
                counter+=1
            except Exception as e:
                # Log error but continue processing other games
                print(f"Error processing game {game.id}: {str(e)}")
                continue
        
        return processed_games

    async def get_game(self, game_id: int, move_notation: str = 'san') -> Optional[GameResponse]:
        """
        Retrieve a single game by ID with move notation conversion.
        
        Args:
            game_id: ID of the game to retrieve
            move_notation: Desired move notation format ('uci' or 'san')
            
        Returns:
            GameResponse object if found, None otherwise
        """
        result = await self.db.execute(
            select(GameDB)
            .options(
                joinedload(GameDB.white_player),
                joinedload(GameDB.black_player)
            )
            .filter(GameDB.id == game_id)
        )
        
        game = result.scalar_one_or_none()
        if not game:
            return None
            
        try:
            # Decode binary game data
            moves = self.decoder.decode_moves(game.moves)
            if move_notation == 'san':
                moves = self.decoder.convert_uci_to_san(moves)
            
            return GameResponse(
                id=game.id,
                white_player_id=game.white_player_id,
                black_player_id=game.black_player_id,
                white_player=game.white_player,
                black_player=game.black_player,
                date=game.date,
                result=game.result,
                eco=game.eco,
                moves=' '.join(moves)
            )
            
        except Exception as e:
            print(f"Error processing game {game_id}: {str(e)}")
            return None

    async def get_player_games(
        self,
        player_id: int,
        move_notation: str = 'san'
    ) -> List[GameResponse]:
        """
        Retrieve all games for a specific player.
        
        Args:
            player_id: ID of the player
            move_notation: Desired move notation format ('uci' or 'san')
            
        Returns:
            List of GameResponse objects for the player
        """
        result = await self.db.execute(
            select(GameDB)
            .options(
                joinedload(GameDB.white_player),
                joinedload(GameDB.black_player)
            )
            .where(
                or_(
                    GameDB.white_player_id == player_id,
                    GameDB.black_player_id == player_id
                )
            )
            .order_by(GameDB.date.desc())
        )
        
        games = result.scalars().unique().all()
        
        processed_games = []
        for game in games:
            try:
                moves = self.decoder.decode_moves(game.moves)
                
                if move_notation == 'san':
                    moves = self.decoder.convert_uci_to_san(moves)
                
                processed_game = GameResponse(
                    id=game.id,
                    white_player_id=game.white_player_id,
                    black_player_id=game.black_player_id,
                    white_player=game.white_player,
                    black_player=game.black_player,
                    date=game.date,
                    result=game.result,
                    eco=game.eco,
                    moves=' '.join(moves)
                )
                processed_games.append(processed_game)
                
            except Exception as e:
                print(f"Error processing game {game.id}: {str(e)}")
                continue
                
        return processed_games

    async def get_game_stats(self):
        # Get basic statistics about the games
        result = await self.db.execute("""
            SELECT 
                COUNT(*) as total_games,
                COUNT(DISTINCT white_player_id) + COUNT(DISTINCT black_player_id) as total_players,
                MIN(date) as earliest_game,
                MAX(date) as latest_game
            FROM games
        """)
        stats = await result.first()
        return {
            "total_games": stats[0],
            "total_players": stats[1],
            "earliest_game": stats[2],
            "latest_game": stats[3]
        }
    

from typing import List, Dict, Optional, TypedDict, Tuple
import logging
from datetime import datetime
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.exc import SQLAlchemyError
from modules.ops.encode import ChessMoveEncoder

class MoveCountStats(TypedDict):
    """Type definition for move count statistics"""
    actual_full_moves: int           # Number of full moves in game
    number_of_games: int            # Count of games with this move count
    avg_bytes: float               # Average size of encoded game data
    results: str                   # Aggregated game results
    min_stored_count: Optional[int] # Minimum stored move count
    max_stored_count: Optional[int] # Maximum stored move count
    avg_stored_count: float        # Average stored move count

class PlayerPerformance(TypedDict):
    """Type definition for player performance statistics"""
    time_period: str              # Time period (month/year)
    games_played: int            # Total games played
    wins: int                    # Number of wins
    losses: int                  # Number of losses
    draws: int                   # Number of draws
    win_rate: float             # Win percentage
    avg_moves: float            # Average moves per game
    white_games: int            # Games played as white
    black_games: int            # Games played as black
    elo_rating: Optional[int]    # ELO rating if available

class OpeningStats(TypedDict):
    """Type definition for opening statistics"""
    eco_code: str               # ECO code of the opening
    opening_name: str           # Name of the opening
    games_played: int          # Number of games with this opening
    win_rate: float           # Win rate with this opening
    avg_moves: float          # Average game length with this opening

class AnalysisRepository:
    """Repository for analyzing chess game statistics with focus on move data analysis"""
    
    def __init__(self, db: AsyncSession):
        """
        Initialize repository with database session
        
        Args:
            db (AsyncSession): Async SQLAlchemy session for database operations
        """
        self.db = db
        self.logger = logging.getLogger(f"{__name__}.AnalysisRepository")
        self.move_encoder = ChessMoveEncoder()  # For decoding move data if needed


    async def get_player_performance_timeline(
        self,
        player_id: int,
        time_range: str = "monthly",  # 'monthly' or 'yearly'
        start_date: Optional[str] = None,
        end_date: Optional[str] = None
    ) -> List[PlayerPerformance]:
        """
        Analyze a player's performance over time.
        
        Args:
            player_id: Database ID of the player
            time_range: Aggregation period ('monthly' or 'yearly')
            start_date: Optional start date for analysis (YYYY-MM-DD)
            end_date: Optional end date for analysis (YYYY-MM-DD)
            
        Returns:
            List[PlayerPerformance]: Performance statistics over time
            
        Raises:
            ValueError: If player_id is invalid or dates are malformed
            SQLAlchemyError: On database operation failures
        """
        try:
            # Validate time range
            if time_range not in ('monthly', 'yearly'):
                raise ValueError("time_range must be 'monthly' or 'yearly'")

            # Construct date grouping expression based on time_range
            date_group = (
                "DATE_TRUNC('month', date)" if time_range == 'monthly'
                else "DATE_TRUNC('year', date)"
            )

            query = text(f"""
                WITH player_games AS (
                    SELECT
                        {date_group} as period,
                        date,
                        CASE
                            WHEN white_player_id = %(player_id)s THEN 'white'
                            ELSE 'black'
                        END as player_color,
                        CASE
                            WHEN white_player_id = %(player_id)s THEN white_elo
                            ELSE black_elo
                        END as player_elo,
                        result,
                        (octet_length(moves) - 19) / 2 as move_count,
                        CASE
                            WHEN (white_player_id = %(player_id)s AND result = '1-0') OR
                                 (black_player_id = %(player_id)s AND result = '0-1')
                                THEN 1
                            ELSE 0
                        END as is_win,
                        CASE
                            WHEN (white_player_id = %(player_id)s AND result = '0-1') OR
                                 (black_player_id = %(player_id)s AND result = '1-0')
                                THEN 1
                            ELSE 0
                        END as is_loss,
                        CASE
                            WHEN result = '1/2-1/2' THEN 1
                            ELSE 0
                        END as is_draw
                    FROM games
                    WHERE (white_player_id = %(player_id)s OR black_player_id = %(player_id)s)
                    AND date IS NOT NULL
                    AND date >= COALESCE(%(start_date)s::date, '1900-01-01')
                    AND date <= COALESCE(%(end_date)s::date, CURRENT_DATE)
                )
                SELECT
                    period::date as time_period,
                    COUNT(*) as games_played,
                    SUM(is_win) as wins,
                    SUM(is_loss) as losses,
                    SUM(is_draw) as draws,
                    ROUND(AVG(move_count)::numeric, 2) as avg_moves,
                    SUM(CASE WHEN player_color = 'white' THEN 1 ELSE 0 END) as white_games,
                    SUM(CASE WHEN player_color = 'black' THEN 1 ELSE 0 END) as black_games,
                    ROUND(AVG(player_elo)::numeric, 0) as elo_rating,
                    ROUND((SUM(is_win)::float / COUNT(*) * 100)::numeric, 2) as win_rate
                FROM player_games
                GROUP BY period
                ORDER BY period ASC;
            """)

            result = await self.db.execute(
                query,
                {
                    "player_id": player_id,
                    "start_date": start_date,
                    "end_date": end_date
                }
            )
            
            rows = result.fetchall()
            
            # Process and validate results
            performance_data: List[PlayerPerformance] = []
            
            for row in rows:
                try:
                    stats = PlayerPerformance(
                        time_period=row[0].strftime(
                            '%Y-%m' if time_range == 'monthly' else '%Y'
                        ),
                        games_played=int(row[1]),
                        wins=int(row[2]),
                        losses=int(row[3]),
                        draws=int(row[4]),
                        avg_moves=float(row[5]),
                        white_games=int(row[6]),
                        black_games=int(row[7]),
                        elo_rating=int(row[8]) if row[8] is not None else None,
                        win_rate=float(row[9])
                    )
                    
                    # Validate statistics
                    if (stats['wins'] + stats['losses'] + stats['draws'] 
                            != stats['games_played']):
                        self.logger.warning(
                            f"Data inconsistency for period {stats['time_period']}"
                        )
                        continue
                        
                    if stats['white_games'] + stats['black_games'] != stats['games_played']:
                        self.logger.warning(
                            f"Color distribution mismatch for period {stats['time_period']}"
                        )
                        continue
                        
                    performance_data.append(stats)
                    
                except (TypeError, ValueError) as e:
                    self.logger.warning(
                        f"Error processing performance data: {e}",
                        exc_info=True
                    )
                    continue

            return performance_data

        except SQLAlchemyError as e:
            error_time = datetime.utcnow().isoformat()
            self.logger.error(
                "Player performance analysis failed",
                extra={
                    "timestamp": error_time,
                    "player_id": player_id,
                    "error": str(e)
                },
                exc_info=True
            )
            raise ValueError(f"Error analyzing player performance: {str(e)}")

    async def get_player_opening_stats(
        self,
        player_id: int,
        min_games: int = 5
    ) -> List[OpeningStats]:
        """
        Analyze a player's performance with different openings.
        
        Args:
            player_id: Database ID of the player
            min_games: Minimum number of games with an opening to include in analysis
            
        Returns:
            List[OpeningStats]: Statistics for each opening played by the player
            
        Raises:
            ValueError: If player_id is invalid or min_games is negative
            SQLAlchemyError: On database operation failures
        """
        if min_games < 1:
            raise ValueError("min_games must be positive")

        try:
            query = text("""
                WITH player_openings AS (
                    SELECT
                        eco,
                        COUNT(*) as games_count,
                        ROUND(AVG((octet_length(moves) - 19) / 2)::numeric, 2) as avg_moves,
                        SUM(CASE
                            WHEN (white_player_id = :player_id AND result = '1-0') OR
                                 (black_player_id = :player_id AND result = '0-1')
                                THEN 1
                            ELSE 0
                        END)::float / COUNT(*) * 100 as win_rate
                    FROM games
                    WHERE (white_player_id = :player_id OR black_player_id = :player_id)
                        AND eco IS NOT NULL
                    GROUP BY eco
                    HAVING COUNT(*) >= :min_games
                )
                SELECT
                    po.eco as eco_code,
                    po.games_count as games_played,
                    po.win_rate,
                    po.avg_moves
                FROM player_openings po
                ORDER BY po.games_count DESC, po.win_rate DESC;
            """)

            result = await self.db.execute(
                query,
                {
                    "player_id": player_id,
                    "min_games": min_games
                }
            )
            
            rows = result.fetchall()
            
            # Process and validate results
            opening_stats: List[OpeningStats] = []
            
            for row in rows:
                try:
                    stats = OpeningStats(
                        eco_code=str(row[0]),
                        opening_name=self._get_opening_name(row[0]),  # Implement this helper
                        games_played=int(row[1]),
                        win_rate=float(row[2]),
                        avg_moves=float(row[3])
                    )
                    
                    # Validate statistics
                    if not (0 <= stats['win_rate'] <= 100):
                        self.logger.warning(
                            f"Invalid win rate for ECO {stats['eco_code']}: {stats['win_rate']}"
                        )
                        continue
                        
                    if stats['games_played'] < min_games:
                        continue
                        
                    opening_stats.append(stats)
                    
                except (TypeError, ValueError) as e:
                    self.logger.warning(
                        f"Error processing opening stats: {e}",
                        exc_info=True
                    )
                    continue

            return opening_stats

        except SQLAlchemyError as e:
            error_time = datetime.utcnow().isoformat()
            self.logger.error(
                "Opening analysis failed",
                extra={
                    "timestamp": error_time,
                    "player_id": player_id,
                    "error": str(e)
                },
                exc_info=True
            )
            raise ValueError(f"Error analyzing opening statistics: {str(e)}")
            
    def _get_opening_name(self, eco_code: str) -> str:
        """
        Get opening name from ECO code. This is a placeholder - you would want to
        implement this with a proper opening database or lookup table.
        """
        # TODO: Implement proper ECO code to opening name mapping
        return eco_code  # Return ECO code as placeholder
        try:
            # Define analysis query with explicit column types and data validation
            query = text("""
                WITH game_moves_analysis AS (
                    SELECT
                        -- Calculate actual moves from binary data length
                        -- Subtract header size (19 bytes) and divide remaining by 2 bytes per move
                        (octet_length(moves) - 19) / 2 as actual_full_moves,
                        
                        -- Extract stored move count from header (bytes 17-18)
                        get_byte(moves, 17) << 8 | get_byte(moves, 18) as stored_move_count,
                        
                        -- Game metadata
                        result,
                        octet_length(moves) as total_bytes
                        
                    FROM games
                    WHERE moves IS NOT NULL
                        AND octet_length(moves) >= 19  -- Ensure minimum header size
                )
                SELECT
                    actual_full_moves,
                    COUNT(*) as number_of_games,
                    ROUND(AVG(total_bytes)::numeric, 2) as avg_bytes,
                    string_agg(DISTINCT result, ', ' ORDER BY result) as results,
                    MIN(stored_move_count) as min_stored_count,
                    MAX(stored_move_count) as max_stored_count,
                    ROUND(AVG(stored_move_count)::numeric, 2) as avg_stored_count
                    
                FROM game_moves_analysis
                WHERE 
                    -- Filter out invalid move counts
                    actual_full_moves >= 0
                    AND actual_full_moves <= 500  -- Reasonable maximum game length
                    
                GROUP BY actual_full_moves
                ORDER BY actual_full_moves ASC;
            """)

            # Execute query with explicit transaction handling
            result = self.db.execute(query)
            raw_rows = result.fetchall()

            # Process and validate results
            processed_results: List[MoveCountStats] = []
            
            for row in raw_rows:
                try:
                    # Validate numeric fields
                    actual_moves = int(row[0])
                    num_games = int(row[1])
                    avg_bytes = float(row[2])
                    min_count = int(row[4]) if row[4] is not None else None
                    max_count = int(row[5]) if row[5] is not None else None
                    avg_count = float(row[6]) if row[6] is not None else 0.0

                    # Validate value ranges
                    if not (0 <= actual_moves <= 500):
                        self.logger.warning(
                            f"Invalid move count detected: {actual_moves}"
                        )
                        continue

                    if num_games <= 0:
                        self.logger.warning(
                            f"Invalid game count: {num_games} for {actual_moves} moves"
                        )
                        continue

                    processed_row = MoveCountStats(
                        actual_full_moves=actual_moves,
                        number_of_games=num_games,
                        avg_bytes=avg_bytes,
                        results=str(row[3]),
                        min_stored_count=min_count,
                        max_stored_count=max_count,
                        avg_stored_count=avg_count
                    )
                    processed_results.append(processed_row)

                except (TypeError, ValueError) as e:
                    self.logger.warning(
                        f"Error processing row: {row}",
                        exc_info=e
                    )
                    continue

            # Validate final result set
            if not processed_results:
                self.logger.warning("No valid move count data found")
                return []

            return processed_results

        except SQLAlchemyError as e:
            error_time = datetime.utcnow().isoformat()
            self.logger.error(
                "Move count analysis failed",
                extra={
                    "timestamp": error_time,
                    "error_type": type(e).__name__,
                    "error_details": str(e)
                },
                exc_info=True
            )
            raise ValueError(f"Error analyzing move count distribution: {str(e)}")

    async def get_move_count_distribution(self) -> List[MoveCountStats]:
        """
        Analyze the distribution of move counts across chess games.
        
        This method computes statistics about game lengths, considering both the
        actual encoded moves and the stored move count in the header.
        
        Returns:
            List[MoveCountStats]: Array of move count statistics
            
        Raises:
            SQLAlchemyError: On database operation failures
            ValueError: If data validation fails
        """
        try:
            # Define analysis query with explicit column types and data validation
            query = text("""
                WITH game_moves_analysis AS (
                    SELECT
                        -- Calculate actual moves from binary data length
                        -- Subtract header size (19 bytes) and divide remaining by 2 bytes per move
                        (octet_length(moves) - 19) / 2 as actual_full_moves,
                        
                        -- Extract stored move count from header (bytes 17-18)
                        get_byte(moves, 17) << 8 | get_byte(moves, 18) as stored_move_count,
                        
                        -- Game metadata
                        result,
                        octet_length(moves) as total_bytes
                        
                    FROM games
                    WHERE moves IS NOT NULL
                        AND octet_length(moves) >= 19  -- Ensure minimum header size
                )
                SELECT
                    actual_full_moves,
                    COUNT(*) as number_of_games,
                    ROUND(AVG(total_bytes)::numeric, 2) as avg_bytes,
                    string_agg(DISTINCT result, ', ' ORDER BY result) as results,
                    MIN(stored_move_count) as min_stored_count,
                    MAX(stored_move_count) as max_stored_count,
                    ROUND(AVG(stored_move_count)::numeric, 2) as avg_stored_count
                    
                FROM game_moves_analysis
                WHERE 
                    -- Filter out invalid move counts
                    actual_full_moves >= 0
                    AND actual_full_moves <= 500  -- Reasonable maximum game length
                    
                GROUP BY actual_full_moves
                ORDER BY actual_full_moves ASC;
            """)

            # Execute query with explicit transaction handling
            result = await self.db.execute(query)
            raw_rows = result.fetchall()

            # Process and validate results
            processed_results: List[MoveCountStats] = []
            
            for row in raw_rows:
                try:
                    # Validate numeric fields
                    actual_moves = int(row[0])
                    num_games = int(row[1])
                    avg_bytes = float(row[2])
                    min_count = int(row[4]) if row[4] is not None else None
                    max_count = int(row[5]) if row[5] is not None else None
                    avg_count = float(row[6]) if row[6] is not None else 0.0

                    # Validate value ranges
                    if not (0 <= actual_moves <= 500):
                        self.logger.warning(
                            f"Invalid move count detected: {actual_moves}"
                        )
                        continue

                    if num_games <= 0:
                        self.logger.warning(
                            f"Invalid game count: {num_games} for {actual_moves} moves"
                        )
                        continue

                    processed_row = MoveCountStats(
                        actual_full_moves=actual_moves,
                        number_of_games=num_games,
                        avg_bytes=avg_bytes,
                        results=str(row[3]),
                        min_stored_count=min_count,
                        max_stored_count=max_count,
                        avg_stored_count=avg_count
                    )
                    processed_results.append(processed_row)

                except (TypeError, ValueError) as e:
                    self.logger.warning(
                        f"Error processing row: {row}",
                        exc_info=e
                    )
                    continue

            # Validate final result set
            if not processed_results:
                self.logger.warning("No valid move count data found")
                return []

            return processed_results

        except SQLAlchemyError as e:
            error_time = datetime.utcnow().isoformat()
            self.logger.error(
                "Move count analysis failed",
                extra={
                    "timestamp": error_time,
                    "error_type": type(e).__name__,
                    "error_details": str(e)
                },
                exc_info=True
            )
            raise ValueError(f"Error analyzing move count distribution: {str(e)}")
### End of ./backend/repository.py

### File: ./init/init_db.py
### Size: 4570 bytes
### Content:
# reinit_db.py
import asyncio
import asyncpg
from typing import Optional
import logging
from datetime import datetime

class DatabaseInitializer:
    """Handles PostgreSQL database reinitialization with proper cleanup and setup."""
    
    def __init__(
        self,
        host: str = "localhost",
        port: int = 5434,
        user: str = "postgres",
        password: str = "chesspass",
        database: str = "chess"
    ):
        """
        Initialize database connection parameters.
        
        Args:
            host: Database host address
            port: Database port number
            user: Database username
            password: Database password
            database: Database name to be (re)created
        """
        self.host = host
        self.port = port
        self.user = user
        self.password = password
        self.database = database
        
        # Configure logging
        self.logger = self._setup_logger()
        
    def _setup_logger(self) -> logging.Logger:
        """Configure logging with timestamp and appropriate format."""
        logger = logging.getLogger("DatabaseInitializer")
        logger.setLevel(logging.INFO)
        
        # Create console handler with formatting
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger

    async def _connect_postgres(self) -> asyncpg.Connection:
        """
        Connect to PostgreSQL server using default 'postgres' database.
        
        Returns:
            asyncpg.Connection: Database connection
            
        Raises:
            Exception: If connection fails
        """
        try:
            return await asyncpg.connect(
                host=self.host,
                port=self.port,
                user=self.user,
                password=self.password,
                database='postgres'  # Connect to default database
            )
        except Exception as e:
            self.logger.error(f"Failed to connect to PostgreSQL: {str(e)}")
            raise

    async def reinitialize(self) -> bool:
        """
        Reinitialize the database by dropping and recreating it.
        
        Returns:
            bool: True if reinitialization was successful
            
        Note:
            This will destroy all existing data in the database.
        """
        start_time = datetime.now()
        self.logger.info(f"Starting database reinitialization: {self.database}")
        
        try:
            # Connect to default postgres database
            conn = await self._connect_postgres()
            
            try:
                # Terminate existing connections
                self.logger.info("Terminating existing connections...")
                await conn.execute(f'''
                    SELECT pg_terminate_backend(pg_stat_activity.pid)
                    FROM pg_stat_activity
                    WHERE pg_stat_activity.datname = '{self.database}'
                    AND pid <> pg_backend_pid();
                ''')
                
                # Drop database if exists
                self.logger.info(f"Dropping database if exists: {self.database}")
                await conn.execute(f'DROP DATABASE IF EXISTS {self.database}')
                
                # Create fresh database
                self.logger.info(f"Creating new database: {self.database}")
                await conn.execute(f'CREATE DATABASE {self.database}')
                
                elapsed_time = (datetime.now() - start_time).total_seconds()
                self.logger.info(
                    f"Database reinitialization completed successfully in {elapsed_time:.2f} seconds"
                )
                return True
                
            finally:
                # Always close the connection
                await conn.close()
                
        except Exception as e:
            self.logger.error(f"Database reinitialization failed: {str(e)}")
            return False

async def main():
    """Main execution function with error handling."""
    initializer = DatabaseInitializer()
    
    try:
        success = await initializer.reinitialize()
        if not success:
            exit(1)
    except Exception as e:
        logging.error(f"Fatal error: {str(e)}")
        exit(1)

if __name__ == "__main__":
    asyncio.run(main())
### End of ./init/init_db.py

### File: ./archive/backend/scrape.py
### Size: 25303 bytes
### Content:
import requests
from bs4 import BeautifulSoup
import os
import wget
import zipfile
import tempfile
from tqdm import tqdm
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import hashlib
import json
from pathlib import Path
import time
import sys
from typing import Optional, List, Dict, Any
from dataclasses import dataclass
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from parse import ChessProcessor
import chess
from config import DatabaseConfig
from contextlib import contextmanager


@dataclass
class PGNFile:
    url: str
    filename: str
    description: str
    file_size: Optional[int] = None
    hash: Optional[str] = None
    downloaded_at: Optional[str] = None
    processed_at: Optional[str] = None
    status: str = "pending"

class PGNScraper:
    def __init__(self, db_config: Optional[DatabaseConfig] = None, download_dir: Optional[str] = None):
        """
        Initialize enhanced scraper with improved logging and caching
        
        Args:
            db_config: PostgreSQL database configuration
            download_dir: Optional directory for downloads (uses temp dir if not specified)
        """
        self.db_config = db_config or DatabaseConfig()
        self.base_url = "https://www.pgnmentor.com"
        self.download_dir = Path(download_dir) if download_dir else Path(tempfile.mkdtemp())
        self.cache_dir = self.download_dir / ".cache"
        self.metadata_file = self.cache_dir / "metadata.json"
        
        # Initialize directories
        self._init_directories()
        
        # Set up logging
        self._setup_logging()
        
        # Initialize HTTP session
        self.session = self._setup_http_session()
        
        # Initialize parser with database configuration
        self.parser = ChessProcessor(db_config=self.db_config)
        
        # Load metadata
        self.metadata = self._load_metadata()

    @contextmanager
    def _get_db_connection(self):
        """Delegate database connection to parser"""
        with self.parser._get_db_connection() as conn:
            yield conn

    def parse_pgn_file(self, pgn_path: Path):
        """Delegate PGN parsing to parser"""
        return self.parser.parse_pgn_file(pgn_path)

    def process_file(self, pgn_file: PGNFile) -> bool:
        """Process a single PGN file with comprehensive error handling"""
        try:
            self.logger.info(f"Processing: {pgn_file.description} ({pgn_file.filename})")
            
            # Download file
            filepath = self.download_file(pgn_file)
            if not filepath:
                pgn_file.status = "failed"
                return False
            
            # Extract if it's a ZIP file
            if pgn_file.filename.endswith('.zip'):
                pgn_files = self.extract_zip(filepath)
                if not pgn_files:
                    pgn_file.status = "failed"
                    return False
                
                # Process each PGN file
                success = True
                for pgn_path in pgn_files:
                    try:
                        games_processed, failed = self.parser.parse_pgn_file(pgn_path)
                        if games_processed == 0:
                            success = False
                        self.logger.info(
                            f"Parsed {games_processed} games from {pgn_path.name} "
                            f"(Failed: {failed})"
                        )
                    except Exception as e:
                        self.logger.error(f"Error parsing {pgn_path.name}: {str(e)}")
                        success = False
                
                pgn_file.status = "completed" if success else "partial"
            else:
                # Process single PGN file
                try:
                    games_processed, failed = self.parser.parse_pgn_file(filepath)
                    pgn_file.status = "completed" if games_processed > 0 else "failed"
                except Exception as e:
                    self.logger.error(f"Error parsing {filepath.name}: {str(e)}")
                    pgn_file.status = "failed"
                    return False
            
            # Update metadata
            pgn_file.processed_at = datetime.now().isoformat()
            self._save_metadata()
            
            return pgn_file.status == "completed"
            
        except Exception as e:
            self.logger.error(f"Error processing {pgn_file.filename}: {str(e)}")
            pgn_file.status = "failed"
            self._save_metadata()
            return False
              
    def _init_directories(self):
        """Initialize necessary directories"""
        self.download_dir.mkdir(parents=True, exist_ok=True)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
    def _setup_logging(self):
        """Set up enhanced logging configuration"""
        log_file = self.download_dir / "pgn_scraper.log"
        
        # Create formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - [%(name)s] - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        # File handler with rotation
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        
        # Console handler
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        
        # Configure root logger
        logger = logging.getLogger()
        logger.setLevel(logging.INFO)
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        self.logger = logging.getLogger("PGNScraper")
    
    def _setup_http_session(self) -> requests.Session:
        """Configure HTTP session with retry strategy"""
        session = requests.Session()
        
        # Configure retry strategy
        retries = Retry(
            total=5,
            backoff_factor=0.1,
            status_forcelist=[500, 502, 503, 504]
        )
        
        # Mount adapter with retry strategy
        adapter = HTTPAdapter(max_retries=retries)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        return session
    
    def _load_metadata(self) -> Dict[str, PGNFile]:
        """Load cached metadata"""
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r') as f:
                data = json.load(f)
                return {
                    url: PGNFile(**file_data)
                    for url, file_data in data.items()
                }
        return {}
    
    def _save_metadata(self):
        """Save metadata to cache"""
        with open(self.metadata_file, 'w') as f:
            json.dump(
                {url: vars(file_info) for url, file_info in self.metadata.items()},
                f,
                indent=2
            )
    
    def _calculate_file_hash(self, filepath: Path) -> str:
        """Calculate SHA-256 hash of file"""
        sha256_hash = hashlib.sha256()
        with open(filepath, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    
    def get_pgn_links(self) -> List[PGNFile]:
        """Scrape PGN file links with enhanced error handling"""
        self.logger.info("Fetching PGN links from website")
        
        try:
            response = self.session.get(f"{self.base_url}/files.html")
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            links = []
            
            for a in soup.find_all('a', href=True):
                href = a['href']
                if href.endswith(('.zip', '.pgn')):
                    url = f"{self.base_url}/{href}"
                    
                    # Check if we have cached metadata
                    if url in self.metadata:
                        pgn_file = self.metadata[url]
                    else:
                        pgn_file = PGNFile(
                            url=url,
                            filename=os.path.basename(href),
                            description=a.get_text(strip=True)
                        )
                        self.metadata[url] = pgn_file
                    
                    links.append(pgn_file)
            
            self.logger.info(f"Found {len(links)} PGN files")
            self._save_metadata()
            return links
            
        except requests.RequestException as e:
            self.logger.error(f"Error fetching PGN links: {str(e)}")
            return []
    
    def download_file(self, pgn_file: PGNFile) -> Optional[Path]:
        """Download file with enhanced progress tracking and validation"""
        filepath = self.download_dir / pgn_file.filename
        
        try:
            # Check if file exists and is valid
            if filepath.exists():
                current_hash = self._calculate_file_hash(filepath)
                if pgn_file.hash and current_hash == pgn_file.hash:
                    self.logger.info(f"File already exists and is valid: {pgn_file.filename}")
                    return filepath
            
            # Download with progress tracking
            self.logger.info(f"Downloading: {pgn_file.filename}")
            response = self.session.get(pgn_file.url, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            
            with open(filepath, 'wb') as f, tqdm(
                desc=pgn_file.filename,
                total=total_size,
                unit='iB',
                unit_scale=True
            ) as pbar:
                for data in response.iter_content(chunk_size=1024):
                    size = f.write(data)
                    pbar.update(size)
            
            # Update metadata
            pgn_file.file_size = total_size
            pgn_file.hash = self._calculate_file_hash(filepath)
            pgn_file.downloaded_at = datetime.now().isoformat()
            self._save_metadata()
            
            return filepath
            
        except Exception as e:
            self.logger.error(f"Error downloading {pgn_file.filename}: {str(e)}")
            if filepath.exists():
                filepath.unlink()
            return None
    
    
    def extract_zip(self, zip_path: Path) -> List[Path]:
        """Extract ZIP file with improved error handling"""
        try:
            extract_dir = self.download_dir / 'extracted'
            extract_dir.mkdir(exist_ok=True)
            
            self.logger.info(f"Extracting: {zip_path.name}")
            
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(extract_dir)
            
            # Return list of extracted PGN files
            pgn_files = list(extract_dir.glob('**/*.pgn'))
            self.logger.info(f"Extracted {len(pgn_files)} PGN files from {zip_path.name}")
            
            return pgn_files
            
        except Exception as e:
            self.logger.error(f"Error extracting {zip_path.name}: {str(e)}")
            return []
    
    def parse_pgn_file(self, pgn_path: Path):
        """
        Parse PGN file and store games in database with optimized batch processing.
        
        Args:
            pgn_path: Path to PGN file to parse
            
        Returns:
            Tuple[int, int]: (successfully_parsed_games, failed_games)
        """
        start_time = time.time()
        games_processed = 0
        failed_games = 0
        batch = []
        BATCH_SIZE = 1000  # Adjust based on memory constraints
        
        self.logger.info(f"Starting to parse: {pgn_path}")
        
        try:
            with self._get_db_connection() as conn:
                cursor = conn.cursor()
                
                with open(pgn_path) as pgn_file:
                    while True:
                        try:
                            # Read game from PGN file
                            game = chess.pgn.read_game(pgn_file)
                            if game is None:  # End of file
                                break
                            
                            # Extract game metadata
                            headers = game.headers
                            
                            # Get player IDs (cached operation)
                            white_id = self._get_player_id(
                                headers.get("White", "Unknown"), 
                                cursor
                            )
                            black_id = self._get_player_id(
                                headers.get("Black", "Unknown"), 
                                cursor
                            )
                            
                            # Convert moves to algebraic notation
                            moves_str = " ".join(
                                move.uci() for move in game.mainline_moves()
                            )
                            
                            # Parse Elo ratings
                            white_elo = headers.get("WhiteElo", "")
                            black_elo = headers.get("BlackElo", "")
                            
                            # Clean Elo values
                            white_elo = int(white_elo) if white_elo.isdigit() else None
                            black_elo = int(black_elo) if black_elo.isdigit() else None
                            
                            # Prepare game data tuple
                            game_data = (
                                white_id,
                                black_id,
                                headers.get("Event", ""),
                                headers.get("Site", ""),
                                self._parse_date(headers.get("Date", "")),
                                headers.get("Round", ""),
                                headers.get("Result", ""),
                                white_elo,
                                black_elo,
                                headers.get("ECO", ""),
                                moves_str
                            )
                            
                            batch.append(game_data)
                            games_processed += 1
                            
                            # Process batch when it reaches the size limit
                            if len(batch) >= BATCH_SIZE:
                                self._insert_games_batch(cursor, batch)
                                batch = []  # Clear batch after insertion
                                
                                # Log progress
                                elapsed = time.time() - start_time
                                rate = games_processed / elapsed
                                self.logger.info(
                                    f"Processed {games_processed} games "
                                    f"({rate:.2f} games/second)"
                                )
                                
                        except chess.pgn.SkipGame:
                            # Skip invalid games
                            failed_games += 1
                            self.logger.warning(
                                f"Skipped invalid game at position {games_processed + failed_games}"
                            )
                            continue
                            
                        except Exception as e:
                            # Log other errors but continue processing
                            failed_games += 1
                            self.logger.error(
                                f"Error processing game at position "
                                f"{games_processed + failed_games}: {str(e)}"
                            )
                            continue
                    
                    # Process remaining games in the last batch
                    if batch:
                        try:
                            self._insert_games_batch(cursor, batch)
                        except Exception as e:
                            self.logger.error(f"Error inserting final batch: {str(e)}")
                            failed_games += len(batch)
        
        except Exception as e:
            self.logger.error(f"Fatal error processing PGN file: {str(e)}")
            raise
        
        finally:
            # Log final statistics
            elapsed_time = time.time() - start_time
            success_rate = (games_processed - failed_games) / games_processed * 100 if games_processed > 0 else 0
            
            self.logger.info(
                f"PGN parsing completed:\n"
                f"Total games processed: {games_processed}\n"
                f"Successfully parsed: {games_processed - failed_games}\n"
                f"Failed to parse: {failed_games}\n"
                f"Success rate: {success_rate:.1f}%\n"
                f"Total time: {elapsed_time:.2f} seconds\n"
                f"Average speed: {games_processed/elapsed_time:.2f} games/second"
            )
        
        return games_processed - failed_games, failed_games

    def cleanup(self):
        """Clean up downloaded and extracted files"""
        try:
            # Save final metadata state
            self._save_metadata()
            
            # Remove downloaded files
            for file in self.download_dir.glob('*'):
                if file.is_file() and file != self.metadata_file:
                    file.unlink()
            
            # Remove extracted directory
            extract_dir = self.download_dir / 'extracted'
            if extract_dir.exists():
                for item in extract_dir.rglob('*'):
                    if item.is_file():
                        item.unlink()
                    elif item.is_dir():
                        item.rmdir()
                extract_dir.rmdir()
            
            self.logger.info("Cleanup completed successfully")
        
        except Exception as e:
            self.logger.error(f"Error during cleanup: {str(e)}")

    def run(self, max_workers: int = 4):
        """Main execution method with enhanced parallel processing"""
        start_time = time.time()
        self.logger.info("Starting PGN scraping process")
        
        try:
            # Get all PGN links
            links = self.get_pgn_links()
            if not links:
                self.logger.error("No PGN links found")
                return
            
            # Process files in parallel
            successful = 0
            failed = 0
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_file = {
                    executor.submit(self.process_file, link): link 
                    for link in links
                }
                
                # Show progress bar
                with tqdm(total=len(links), desc="Processing files") as pbar:
                    for future in as_completed(future_to_file):
                        pgn_file = future_to_file[future]
                        try:
                            if future.result():
                                successful += 1
                            else:
                                failed += 1
                        except Exception as e:
                            self.logger.error(f"Error processing {pgn_file.filename}: {str(e)}")
                            failed += 1
                        pbar.update(1)
            
            # Log final results
            elapsed_time = time.time() - start_time
            self.logger.info(
                f"Processing completed in {elapsed_time:.2f} seconds. "
                f"Successful: {successful}, Failed: {failed}"
            )
            
        except Exception as e:
            self.logger.error(f"Error in main scraper execution: {str(e)}")
        finally:
            self._save_metadata()

from typing import Optional, Dict, List
import argparse
import sys
import tempfile
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from pathlib import Path

def setup_logging(log_file: str = 'pgn_scraper.log') -> logging.Logger:
    """Configure logging with both file and console handlers"""
    logger = logging.getLogger('PGNScraper')
    logger.setLevel(logging.INFO)
    
    # File handler
    file_handler = logging.FileHandler(log_file)
    file_handler.setFormatter(
        logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    )
    logger.addHandler(file_handler)
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(
        logging.Formatter('%(levelname)s - %(message)s')
    )
    logger.addHandler(console_handler)
    
    return logger

def parse_arguments() -> argparse.Namespace:
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='PGN Scraper and Parser')
    parser.add_argument(
        '--db-config',
        type=str,
        default='postgresql://postgres:chesspass@localhost:5433/chess',
        help='Database connection string'
    )
    parser.add_argument(
        '--download-dir',
        type=str,
        default=None,
        help='Directory for downloading PGN files'
    )
    parser.add_argument(
        '--max-workers',
        type=int,
        default=4,
        help='Maximum number of parallel workers'
    )
    parser.add_argument(
        '--cleanup',
        action='store_true',
        help='Clean up downloaded files after processing'
    )
    
    return parser.parse_args()

def main():
    """Main execution function with parallel processing and progress tracking"""
    # Parse arguments
    args = parse_arguments()
    
    # Setup logging
    logger = setup_logging()
    
    try:
        # Initialize download directory
        download_dir = args.download_dir or tempfile.mkdtemp()
        Path(download_dir).mkdir(parents=True, exist_ok=True)
        
        # Create database configuration
        db_config = DatabaseConfig(
            host=os.getenv('DB_HOST', 'localhost'),
            port=int(os.getenv('DB_PORT', 5433)),
            database=os.getenv('DB_NAME', 'chess'),
            user=os.getenv('DB_USER', 'postgres'),
            password=os.getenv('DB_PASSWORD', 'chesspass')
        )
        
        # Create scraper instance
        scraper = PGNScraper(
            db_config=db_config,
            download_dir=download_dir
        )
        
        # Get PGN links
        logger.info("Fetching PGN links...")
        links = scraper.get_pgn_links()
        
        if not links:
            logger.error("No PGN files found to process")
            return
        
        logger.info(f"Found {len(links)} PGN files to process")
        
        # Process files in parallel
        successful = 0
        failed = 0
        
        with ThreadPoolExecutor(max_workers=args.max_workers) as executor:
            future_to_link = {
                executor.submit(scraper.process_file, link): link 
                for link in links
            }
            
            with tqdm(total=len(links), desc="Processing files") as pbar:
                for future in as_completed(future_to_link):
                    link = future_to_link[future]
                    try:
                        if future.result():
                            successful += 1
                            logger.info(f"Successfully processed: {link.filename}")
                        else:
                            failed += 1
                            logger.error(f"Failed to process: {link.filename}")
                    except Exception as e:
                        failed += 1
                        logger.error(f"Error processing {link.filename}: {str(e)}")
                    finally:
                        pbar.update(1)
        
        # Log final results
        logger.info(
            f"\nProcessing completed:\n"
            f"Total files: {len(links)}\n"
            f"Successfully processed: {successful}\n"
            f"Failed: {failed}\n"
            f"Success rate: {(successful/len(links)*100):.1f}%"
        )
        
        # Cleanup if requested
        if args.cleanup:
            logger.info("Cleaning up downloaded files...")
            scraper.cleanup()
            logger.info("Cleanup completed")
    
    except KeyboardInterrupt:
        logger.info("Process interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Fatal error: {str(e)}")
        sys.exit(1)
    finally:
        if args.cleanup and 'scraper' in locals():
            try:
                scraper.cleanup()
            except Exception as e:
                logger.error(f"Error during final cleanup: {str(e)}")

if __name__ == "__main__":
    main()
### End of ./archive/backend/scrape.py

### File: ./archive/backend/parse.py
### Size: 8294 bytes
### Content:
import chess.pgn
import psycopg2
from psycopg2.extras import execute_values
from datetime import datetime
import logging
from pathlib import Path
from typing import Optional, Tuple, Dict
from contextlib import contextmanager
import time
from functools import lru_cache

class ChessProcessor:
    def __init__(self, db_config):
        self.db_config = db_config
        self.setup_logger()
        self._initialize_database()
        self.player_cache: Dict[str, int] = {}

    def setup_logger(self):
        self.logger = logging.getLogger('ChessProcessor')
        self.logger.setLevel(logging.INFO)
        
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)

    def _initialize_database(self):
        with self._get_db_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS players (
                        id SERIAL PRIMARY KEY,
                        name TEXT NOT NULL,
                        first_played DATE,
                        last_played DATE,
                        total_games INTEGER DEFAULT 0,
                        CONSTRAINT unique_player_name UNIQUE (name)
                    );

                    CREATE TABLE IF NOT EXISTS games (
                        id SERIAL PRIMARY KEY,
                        white_player_id INTEGER REFERENCES players(id),
                        black_player_id INTEGER REFERENCES players(id),
                        date DATE,
                        result VARCHAR(10),
                        eco VARCHAR(10),
                        moves TEXT
                    );

                    CREATE INDEX IF NOT EXISTS idx_players_name ON players(name);
                    CREATE INDEX IF NOT EXISTS idx_games_players ON games(white_player_id, black_player_id);
                """)
                conn.commit()

    @contextmanager
    def _get_db_connection(self):
        conn = None
        try:
            conn = psycopg2.connect(
                host=self.db_config.host,
                port=self.db_config.port,
                database=self.db_config.database,
                user=self.db_config.user,
                password=self.db_config.password
            )
            # Set higher isolation level for player operations
            conn.set_session(isolation_level='REPEATABLE READ', autocommit=False)
            yield conn
        finally:
            if conn:
                try:
                    conn.close()
                except Exception:
                    pass

    def _get_or_create_player(self, name: str, conn) -> int:
        """Get or create player with proper concurrency handling"""
        max_retries = 3
        retry_delay = 0.1

        for attempt in range(max_retries):
            try:
                with conn.cursor() as cursor:
                    # First try to select the player
                    cursor.execute(
                        "SELECT id FROM players WHERE name = %s",
                        (name,)
                    )
                    result = cursor.fetchone()
                    
                    if result:
                        return result[0]
                    
                    # Player doesn't exist, create new one
                    cursor.execute("""
                        INSERT INTO players (name)
                        VALUES (%s)
                        ON CONFLICT (name) DO UPDATE
                            SET name = EXCLUDED.name
                        RETURNING id
                    """, (name,))
                    
                    player_id = cursor.fetchone()[0]
                    conn.commit()
                    return player_id

            except psycopg2.Error as e:
                conn.rollback()
                if attempt == max_retries - 1:
                    raise
                time.sleep(retry_delay * (attempt + 1))

        raise RuntimeError("Failed to get or create player after retries")

    def parse_pgn_file(self, pgn_path: Path) -> Tuple[int, int]:
        games_processed = 0
        failed_games = 0
        batch = []
        BATCH_SIZE = 100  # Reduced batch size for better concurrency

        with self._get_db_connection() as conn:
            try:
                with open(pgn_path) as pgn_file:
                    while True:
                        try:
                            game = chess.pgn.read_game(pgn_file)
                            if game is None:
                                break

                            # Process each game in its own transaction
                            game_data = self._process_game(game, conn)
                            if game_data:
                                batch.append(game_data)
                                games_processed += 1

                                if len(batch) >= BATCH_SIZE:
                                    self._insert_games_batch(conn, batch)
                                    batch = []

                        except Exception as e:
                            failed_games += 1
                            self.logger.error(f"Error processing game: {str(e)}")
                            continue

                    # Process remaining games
                    if batch:
                        self._insert_games_batch(conn, batch)

            except Exception as e:
                self.logger.error(f"Error processing file {pgn_path}: {str(e)}")
                raise

        return games_processed, failed_games

    def _process_game(self, game, conn) -> Optional[tuple]:
        """Process single game with proper transaction handling"""
        try:
            headers = game.headers
            
            # Get player IDs with retries
            white_id = self._get_or_create_player(headers.get("White", "Unknown"), conn)
            black_id = self._get_or_create_player(headers.get("Black", "Unknown"), conn)

            moves = []
            board = game.board()
            for move in game.mainline_moves():
                moves.append(move.uci())

            return (
                white_id,
                black_id,
                self._parse_date(headers.get("Date", "")),
                headers.get("Result", "*"),
                headers.get("ECO", ""),
                " ".join(moves)
            )

        except Exception as e:
            self.logger.error(f"Error processing game data: {str(e)}")
            return None

    def _insert_games_batch(self, conn, batch):
        """Insert batch of games with retry logic"""
        if not batch:
            return

        max_retries = 3
        retry_delay = 0.1

        for attempt in range(max_retries):
            try:
                with conn.cursor() as cursor:
                    execute_values(
                        cursor,
                        """
                        INSERT INTO games (
                            white_player_id, black_player_id, date,
                            result, eco, moves
                        ) VALUES %s
                        """,
                        batch,
                        page_size=100
                    )
                conn.commit()
                return
            except psycopg2.Error as e:
                conn.rollback()
                if attempt == max_retries - 1:
                    raise
                time.sleep(retry_delay * (attempt + 1))

    def _parse_date(self, date_str: Optional[str]) -> Optional[str]:
        if not date_str or date_str == "???":
            return None
            
        try:
            for fmt in ("%Y.%m.%d", "%Y/%m/%d", "%Y-%m-%d"):
                try:
                    return datetime.strptime(date_str, fmt).strftime("%Y-%m-%d")
                except ValueError:
                    continue
            
            if len(date_str.replace(".", "")) == 4:
                return f"{date_str.replace('.', '')}-01-01"
            
            return None
        except Exception:
            return None
### End of ./archive/backend/parse.py

# Summary
Processed: 12 files
Total size: 142673 bytes
